{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polyglot Persistence with Blaze  \n",
    "\n",
    "Our world is complex and no single approach exists that solves all problems. Likewise, in the data world one cannot solve all problems with one piece of technology.  \n",
    "\n",
    "Nowadays, any big technology company uses (in one form or another) a MapReduce paradigm to sift through terabytes (or even petabytes) of data collected daily. On the other hand, it is much easier to store, retrieve, extend, and update information about products in a document-type database (such as MongoDB) than it is in a relational database. Yet, persisting transaction records in a relational database aids later data summarizing and reporting.  \n",
    "\n",
    "Even these simple examples show that solving a vast array of business problems requires adapting to different technologies. This means that you, as a database manager, data scientist, or data engineer, would have to learn all of these separately if you were to solve your problems with the tools that are designed to solve them easily. This, however, does not make your company agile and is prone to errors and lots of tweaking and hacking needing to be done to your system.  \n",
    "\n",
    "Blaze abstracts most of the technologies and exposes a simple and elegant data structure and API.  \n",
    "\n",
    "In this chapter, you will learn:  \n",
    "• How to install Blaze  \n",
    "• What polyglot persistence is about  \n",
    "• How to abstract data stored in files, pandas DataFrames, or NumPy arrays  \n",
    "• How to work with archives (GZip)\n",
    "• How to connect to SQL (PostgreSQL and SQLite) and No-SQL (MongoDB)\n",
    "databases with Blaze  \n",
    "• How to query, join, sort, and transform the data, and perform simple\n",
    "summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is now left to do is to import Blaze itself in our notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polyglot persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neal Ford introduced the, somewhat similar, term polyglot programming in 2006.\n",
    "He used it to illustrate the fact that there is no such thing as a one-size-fits-all\n",
    "solution and advocated using multiple programming languages that were more\n",
    "suitable for certain problems.  \n",
    "\n",
    "In the parallel world of data, any business that wants to remain competitive needs to\n",
    "adapt a range of technologies that allows it to solve the problems in a minimal time,\n",
    "thus minimizing the costs.  \n",
    "\n",
    "Storing transactional data in Hadoop files is possible, but makes little sense. On\n",
    "the other hand, processing petabytes of Internet logs using a Relational Database\n",
    "Management System (RDBMS) would also be ill-advised. These tools were\n",
    "designed to tackle specific types of tasks; even though they can be co-opted to solve\n",
    "other problems, the cost of adapting the tools to do so would be enormous. It is a\n",
    "virtual equivalent of trying to fit a square peg in a round hole.  \n",
    "\n",
    "For example, consider a company that sells musical instruments and accessories\n",
    "online (and in a network of shops). At a high-level, there are a number of problems\n",
    "that a company needs to solve to be successful:  \n",
    "* 1. Attract customers to its stores (both virtual and physical).  \n",
    "* Present them with relevant products (you would not try to sell a drum kit to a pianist, would you?!).   \n",
    "* 3. Once they decide to buy, process the payment and organize shipping.  \n",
    "\n",
    "To solve these problems a company might choose from a number of available\n",
    "technologies that were designed to solve these problems:  \n",
    "\n",
    "1. Store all the products in a document-based database such as MongoDB,\n",
    "Cassandra, DynamoDB, or DocumentDB. There are multiple advantages of\n",
    "document databases: flexible schema, sharding (breaking bigger databases\n",
    "into a set of smaller, more manageable ones), high availability, and\n",
    "replication, among others.  \n",
    "\n",
    "2. Model the recommendations using a graph-based database (such as Neo4j,\n",
    "Tinkerpop/Gremlin, or GraphFrames for Spark): such databases reflect the\n",
    "factual and abstract relationships between customers and their preferences.\n",
    "Mining such a graph is invaluable and can produce a more tailored offering\n",
    "for a customer.  \n",
    "\n",
    "3. For searching, a company might use a search-tailored solution such as\n",
    "Apache Solr or ElasticSearch. Such a solution provides fast, indexed text\n",
    "searching capabilities.  \n",
    "\n",
    "4. Once a product is sold, the transaction normally has a well-structured\n",
    "schema (such as product name, price, and so on.) To store such data (and\n",
    "later process and report on it) relational databases are best suited.  \n",
    "\n",
    "With polyglot persistence, a company always chooses the right tool for the right job\n",
    "instead of trying to coerce a single technology into solving all of its problems.  \n",
    "\n",
    "Blaze can abstract many different data structures and expose a single, easy-to-use\n",
    "API. This helps to get a consistent behavior and reduce the need to learn multiple\n",
    "interfaces to handle data. If you know pandas, there is not really that much to learn,\n",
    "as the differences in the syntax are subtle. We will go through some examples to\n",
    "illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "simpleArray = np.array([\n",
    "[1,2,3],\n",
    "[4,5,6]\n",
    "])\n",
    "\n",
    "simpleArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleArray.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    4\n",
       "Name: a, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "simpleDf = pd.DataFrame([\n",
    "[1,2,3],\n",
    "[4,5,6]\n",
    "], columns=['a','b','c'])\n",
    "\n",
    "simpleDf['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "The concepts presented in this chapter are just the beginning of the road to using\n",
    "Blaze. There are many other ways it can be used and data sources it can connect\n",
    "with. Treat this as a starting point to build your understanding of polyglot\n",
    "persistence.  \n",
    "\n",
    "Note, however, that these days most of the concepts explained in this chapter can be\n",
    "attained natively within Spark, as you can use SQLAlchemy directly within Spark\n",
    "making it easy to work with a variety of data sources. The advantage of doing so,\n",
    "despite the initial investment of learning the API of SQLAlchemy, is that the data\n",
    "returned will be stored in a Spark DataFrame and you will have access to everything\n",
    "that PySpark has to offer. This, by no means, implies that you never should never use\n",
    "Blaze: the choice, as always, is yours.  \n",
    "\n",
    "In the next chapter, you will learn about streaming and how to do it with Spark.\n",
    "Streaming has become an increasingly important topic these days, as, daily (true\n",
    "as of 2016), the world produces roughly 2.5 exabytes of data (source: http://www.\n",
    "northeastern.edu/levelblog/2016/05/13/how-much-data-produced-everyday/)\n",
    "that need to be ingested, processed and made sense of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_envi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
