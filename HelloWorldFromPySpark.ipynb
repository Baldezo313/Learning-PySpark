{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Spark  \n",
    "\n",
    "Apache Spark is a powerful open source processing engine originally developed by Matei Zaharia as a part of his PhD thesis while at UC Berkeley. The first version of Spark was released in 2012.  \n",
    "\n",
    "Apache Spark is fast, easy to use framework, that allows you to solve a wide variety of complex data problems whether semi-structured, structured, streaming, and/or machine learning / data sciences.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Apache Spark?  \n",
    "\n",
    "Apache Spark is an open-source powerful distributed querying and processing\n",
    "engine. It provides flexibility and extensibility of MapReduce but at significantly\n",
    "higher speeds: Up to 100 times faster than Apache Hadoop when data is stored in\n",
    "memory and up to 10 times when accessing disk.  \n",
    "\n",
    "Apache Spark allows the user to read, transform, and aggregate data, as well as train\n",
    "and deploy sophisticated statistical models with ease. The Spark APIs are accessible\n",
    "in Java, Scala, Python, R and SQL. Apache Spark can be used to build applications or\n",
    "package them up as libraries to be deployed on a cluster or perform quick analytics\n",
    "interactively through notebooks (like, for instance, Jupyter, Spark-Notebook,\n",
    "Databricks notebooks, and Apache Zeppelin).  \n",
    "\n",
    "Apache Spark exposes a host of libraries familiar to data analysts, data scientists\n",
    "or researchers who have worked with Python's pandas or R's data.frames or\n",
    "data.tables. It is important to note that while Spark DataFrames will be familiar\n",
    "to pandas or data.frames / data.tables users, there are some differences so\n",
    "please temper your expectations. Users with more of a SQL background can use the\n",
    "language to shape their data as well. Also, delivered with Apache Spark are several\n",
    "already implemented and tuned algorithms, statistical models, and frameworks:\n",
    "MLlib and ML for machine learning, GraphX and GraphFrames for graph\n",
    "processing, and Spark Streaming (DStreams and Structured). Spark allows the user\n",
    "to combine these libraries seamlessly in the same application.  \n",
    "\n",
    "Apache Spark can easily run locally on a laptop, yet can also easily be deployed in\n",
    "standalone mode, over YARN, or Apache Mesos - either on your local cluster or\n",
    "in the cloud. It can read and write from a diverse data sources including (but not\n",
    "limited to) HDFS, Apache Cassandra, Apache HBase, and S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Jobs and APIs  \n",
    "\n",
    "* **Execution process**  \n",
    "Any Spark application spins off a single driver process (that can contain multiple\n",
    "jobs) on the *master* node that then directs executor processes (that contain multiple\n",
    "tasks) distributed to a number of worker nodes.  \n",
    "The driver process determines the number and the composition of the task processes\n",
    "directed to the executor nodes based on the graph generated for the given job. Note,\n",
    "that any *worker* node can execute tasks from a number of different jobs.  \n",
    "A Spark job is associated with a chain of object dependencies organized in a direct\n",
    "acyclic graph (DAG) such as the following example generated from the Spark UI.\n",
    "Given this, Spark can optimize the scheduling (for example, determine the number\n",
    "of tasks and workers required) and execution of these tasks.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset  \n",
    "\n",
    "Apache Spark is built around a distributed collection of immutable Java Virtual\n",
    "Machine (JVM) objects called Resilient Distributed Datasets (RDDs for short). As\n",
    "we are working with Python, it is important to note that the Python data is stored\n",
    "within these JVM objects.  \n",
    "These objects allow any job to perform calculations very\n",
    "quickly. RDDs are calculated against, cached, and stored in-memory: a scheme that\n",
    "results in orders of magnitude faster computations compared to other traditional\n",
    "distributed frameworks like Apache Hadoop.\n",
    "\n",
    "At the same time, RDDs expose some coarse-grained transformations (such as\n",
    "map(...), reduce(...), and filter(...)), keeping the flexibility and extensibility of\n",
    "the Hadoop platform to perform a wide variety of calculations. RDDs apply and log\n",
    "transformations to the data in parallel, resulting in both increased speed and faulttolerance.\n",
    "By registering the transformations, RDDs provide data lineage - a form\n",
    "of an ancestry tree for each intermediate step in the form of a graph. This, in effect,\n",
    "guards the RDDs against data loss - if a partition of an RDD is lost it still has enough\n",
    "information to recreate that partition instead of simply depending on replication.  \n",
    "\n",
    "RDDs have two sets of parallel operations: transformations (which return pointers\n",
    "to new RDDs) and actions (which return values to the driver after running a\n",
    "computation);\n",
    "\n",
    "RDD transformation operations are lazy in a sense that they do not compute their\n",
    "results immediately. The transformations are only computed when an action is\n",
    "executed and the results need to be returned to the driver. This delayed execution\n",
    "results in more fine-tuned queries: Queries that are optimized for performance.\n",
    "This optimization starts with Apache Spark's DAGScheduler â€“ the stage oriented\n",
    "scheduler that transforms using stages as seen in the preceding screenshot. By\n",
    "having separate RDD transformations and actions, the DAGScheduler can perform\n",
    "optimizations in the query including being able to avoid shuffling, the data (the most\n",
    "resource intensive task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames  \n",
    "DataFrames, like RDDs, are immutable collections of data distributed among the\n",
    "nodes in a cluster. However, unlike RDDs, in DataFrames data is organized into\n",
    "named columns. [If you are familiar with Python's pandas or R data.frames, this is a\n",
    "similar concept.]  \n",
    "\n",
    "DataFrames were designed to make large data sets processing even easier. They\n",
    "allow developers to formalize the structure of the data, allowing higher-level\n",
    "abstraction; in that sense DataFrames resemble tables from the relational database\n",
    "world. DataFrames provide a domain specific language API to manipulate the\n",
    "distributed data and make Spark accessible to a wider audience, beyond specialized\n",
    "data engineers.\n",
    "\n",
    "\n",
    "One of the major benefits of DataFrames is that the Spark engine initially builds\n",
    "a logical execution plan and executes generated code based on a physical plan\n",
    "determined by a cost optimizer. Unlike RDDs that can be significantly slower on\n",
    "Python compared with Java or Scala, the introduction of DataFrames has brought\n",
    "performance parity across all the languages.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catalyst Optimizer  \n",
    "\n",
    "Spark SQL is one of the most technically involved components of Apache Spark as\n",
    "it powers both SQL queries and the DataFrame API. At the core of Spark SQL is the\n",
    "Catalyst Optimizer. The optimizer is based on functional programming constructs\n",
    "and was designed with two purposes in mind: To ease the addition of new\n",
    "optimization techniques and features to Spark SQL and to allow external developers\n",
    "to extend the optimizer (for example, adding data source specific rules, support for\n",
    "new data types, and so on).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifying Datasets and DataFrames  \n",
    "\n",
    "In the previous section, we stated out that Datasets (at the time of writing this book)\n",
    "are only available in Scala or Java. However, we are providing the following context\n",
    "to better understand the direction of Spark 2.0.\n",
    "Datasets were introduced in 2015 as part of the Apache Spark 1.6 release. The\n",
    "goal for datasets was to provide a type-safe, programming interface. This allowed\n",
    "developers to work with semi-structured data (like JSON or key-value pairs) with\n",
    "compile time type safety (that is, production applications can be checked for errors\n",
    "before they run). Part of the reason why Python does not implement a Dataset API is\n",
    "because Python is not a type-safe language.\n",
    "Just as important, the Datasets API contain high-level domain specific language\n",
    "operations such as sum(), avg(), join(), and group(). This latter trait means\n",
    "that you have the flexibility of traditional Spark RDDs but the code is also easier\n",
    "to express, read, and write. Similar to DataFrames, Datasets can take advantage\n",
    "of Spark's catalyst optimizer by exposing expressions and data fields to a query\n",
    "planner and making use of Tungsten's fast in-memory encoding.  \n",
    "\n",
    "The unification of the DataFrame and Dataset APIs has the potential of creating\n",
    "breaking changes to backwards compatibility. This was one of the main reasons\n",
    "Apache Spark 2.0 was a major release (as opposed to a 1.x minor release which\n",
    "would have minimized any breaking changes). As you can see from the following\n",
    "diagram, DataFrame and Dataset both belong to the new Dataset API introduced\n",
    "as part of Apache Spark 2.0\n",
    "\n",
    "As noted previously, the Dataset API provides a type-safe, object-oriented\n",
    "programming interface. Datasets can take advantage of the Catalyst optimizer by\n",
    "exposing expressions and data fields to the query planner and Project Tungsten's\n",
    "Fast In-memory encoding. But with DataFrame and Dataset now unified as part of\n",
    "Apache Spark 2.0, DataFrame is now an alias for the Dataset Untyped API. More\n",
    "specifically:  \n",
    "DataFrame = Dataset[Row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing SparkSession  \n",
    "\n",
    "In the past, you would potentially work with SparkConf, SparkContext,\n",
    "SQLContext, and HiveContext to execute your various Spark queries for\n",
    "configuration, Spark context, SQL context, and Hive context respectively.\n",
    "The SparkSession is essentially the combination of these contexts including\n",
    "StreamingContext.  \n",
    "\n",
    "***  \n",
    "For example, instead of writing:  \n",
    "df = sqlContext.read.format('json').load('py/test/sql/people.json')  \n",
    "now you can write:  \n",
    "df = spark.read.format('json').load('py/test/sql/people.json')  \n",
    "or:  \n",
    "df = spark.read.json('py/test/sql/people.json')  \n",
    "***  \n",
    "\n",
    "The SparkSession is now the entry point for reading data, working with metadata,\n",
    "configuring the session, and managing the cluster resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming  \n",
    "\n",
    "This is the underlying foundation for building Structured Streaming. While\n",
    "streaming is powerful, one of the key issues is that streaming can be difficult to build\n",
    "and maintain. While companies such as Uber, Netflix, and Pinterest have Spark\n",
    "Streaming applications running in production, they also have dedicated teams to\n",
    "ensure the systems are highly available.\n",
    "As implied previously, there are many things that can go wrong when operating\n",
    "Spark Streaming (and any streaming system for that matter) including (but not\n",
    "limited to) late events, partial outputs to the final data source, state recovery on\n",
    "failure, and/or distributed reads/writes:\n",
    "\n",
    "Therefore, to simplify Spark Streaming, there is now a single API that addresses\n",
    "both batch and streaming within the Apache Spark 2.0 release. More succinctly, the\n",
    "high-level streaming API is now built on top of the Apache Spark SQL Engine. It\n",
    "runs the same queries as you would with Datasets/DataFrames providing you with\n",
    "all the performance and optimization benefits as well as benefits such as event time,\n",
    "windowing, sessions, sources, and sinks.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous applications  \n",
    "Altogether, Apache Spark 2.0 not only unified DataFrames and Datasets but also\n",
    "unified streaming, interactive, and batch queries. This opens a whole new set of use\n",
    "cases including the ability to aggregate data into a stream and then serving it using\n",
    "traditional JDBC/ODBC, to change queries at run time, and/or to build and apply\n",
    "ML models in for many scenario in a variety of latency use cases.  \n",
    "\n",
    "Together, you can now build end-to-end continuous applications, in which you\n",
    "can issue the same queries to batch processing as to real-time data, perform ETL,\n",
    "generate reports, update or track specific data in the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22\n"
     ]
    }
   ],
   "source": [
    "a = 56\n",
    "b = 78\n",
    "print(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_envi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
