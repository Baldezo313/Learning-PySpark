{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the ML Package  \n",
    "\n",
    "In the previous chapter, we worked with the MLlib package in Spark that operated strictly on RDDs. In this chapter, we move to the ML part of Spark that operates strictly on DataFrames. Also, according to the Spark documentation, the primary machine learning API for Spark is now the DataFrame-based set of models contained in the spark.ml package.  \n",
    "\n",
    "In this chapter, you will learn how to do the following:  \n",
    "• Prepare transformers, estimators, and pipelines  \n",
    "• Predict the chances of infant survival using models available in the\n",
    "ML package  \n",
    "• Evaluate the performance of the model  \n",
    "• Perform parameter hyper-tuning  \n",
    "• Use other machine-learning models available in the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.types as typ\n",
    "import findspark\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark.ml.feature as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(\"C:/Program Files/spark-3.5.4-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jre1.8.0_431\" \n",
    "os.environ[\"SPARK_HOME\"] = \"C:/Program Files/spark-3.5.4-bin-hadoop3\" \n",
    "os.environ['HADOOP_HOME '] = 'C:/Program Files/hadoop-3.4.0/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Introducing the ML Package\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # Accès au SparkContext à partir de SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the package  \n",
    "\n",
    "At the top level, the package exposes three main abstract classes: a Transformer, an Estimator, and a Pipeline. We will shortly explain each with some short examples. We will provide more concrete examples of some of the models in the last section of this chapter.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer  \n",
    "The Transformer class, like the name suggests, transforms your data by (normally)\n",
    "appending a new column to your DataFrame.  \n",
    "\n",
    "At the high level, when deriving from the Transformer abstract class, each and\n",
    "every new Transformer needs to implement a .transform(...) method. The\n",
    "method, as a first and normally the only obligatory parameter, requires passing\n",
    "a DataFrame to be transformed. This, of course, varies method-by-method in the\n",
    "ML package: other popular parameters are inputCol and outputCol; these,\n",
    "however, frequently default to some predefined values, such as, for example,\n",
    "'features' for the inputCol parameter.  \n",
    "\n",
    "There are many Transformers offered in the spark.ml.feature and we will briefly\n",
    "describe them here (before we use some of them later in this chapter):  \n",
    "* Binarizer: Given a threshold, the method takes a continuous variable and\n",
    "transforms it into a binary one.  \n",
    "* Bucketizer: Similar to the Binarizer, this method takes a list of thresholds\n",
    "(the splits parameter) and transforms a continuous variable into a\n",
    "multinomial one.  \n",
    "* ChiSqSelector: For the categorical target variables (think classification\n",
    "models), this feature allows you to select a predefined number of features\n",
    "(parameterized by the numTopFeatures parameter) that explain the variance in\n",
    "the target the best. The selection is done, as the name of the method suggests,\n",
    "using a Chi-Square test. It is one of the two-step methods: first, you need\n",
    "to .fit(...) your data (so the method can calculate the Chi-square tests).\n",
    "Calling the .fit(...) method (you pass your DataFrame as a parameter)\n",
    "returns a ChiSqSelectorModel object that you can then use to transform\n",
    "your DataFrame using the .transform(...) method.  \n",
    "* CountVectorizer: This is useful for a tokenized text (such as\n",
    "[['Learning', 'PySpark', 'with', 'us'],['us', 'us', 'us']]).\n",
    "It is one of two-step methods: first, you need to .fit(...), that is, learn\n",
    "the patterns from your dataset, before you can .transform(...) with the\n",
    "CountVectorizerModel returned by the .fit(...) method. The output\n",
    "from this transformer, for the tokenized text presented previously, would\n",
    "look similar to this: [(4, [0, 1, 2, 3], [1.0, 1.0, 1.0, 1.0]),(4,\n",
    "[3], [3.0])].  \n",
    "* DCT: The Discrete Cosine Transform takes a vector of real values and returns\n",
    "a vector of the same length, but with the sum of cosine functions oscillating\n",
    "at different frequencies. Such transformations are useful to extract some\n",
    "underlying frequencies in your data or in data compression.  \n",
    "* ElementwiseProduct: A method that returns a vector with elements that\n",
    "are products of the vector passed to the method, and a vector passed as the\n",
    "scalingVec parameter. For example, if you had a [10.0, 3.0, 15.0]\n",
    "vector and your scalingVec was [0.99, 3.30, 0.66], then the vector\n",
    "you would get would look as follows: [9.9, 9.9, 9.9].  \n",
    "* HashingTF: A hashing trick transformer that takes a list of tokenized text\n",
    "and returns a vector (of predefined length) with counts  \n",
    "* IDF: This method computes an Inverse Document Frequency for a list of\n",
    "documents. Note that the documents need to already be represented as a\n",
    "vector (for example, using either the HashingTF or CountVectorizer).  \n",
    "* IndexToString: A complement to the StringIndexer method. It uses the\n",
    "encoding from the StringIndexerModel object to reverse the string index to\n",
    "original values. As an aside, please note that this sometimes does not work\n",
    "and you need to specify the values from the StringIndexer.  \n",
    "* MaxAbsScaler: Rescales the data to be within the [-1.0, 1.0] range\n",
    "(thus, it does not shift the center of the data).  \n",
    "* MinMaxScaler: This is similar to the MaxAbsScaler with the difference that it\n",
    "scales the data to be in the [0.0, 1.0] range.  \n",
    "* NGram: This method takes a list of tokenized text and returns n-grams: pairs,\n",
    "triples, or n-mores of subsequent words. For example, if you had a ['good',\n",
    "'morning', 'Robin', 'Williams'] vector you would get the following\n",
    "output: ['good morning', 'morning Robin', 'Robin Williams'].  \n",
    "* Normalizer: This method scales the data to be of unit norm using the\n",
    "p-norm value (by default, it is L2).  \n",
    "* OneHotEncoder: This method encodes a categorical column to a column\n",
    "of binary vectors.  \n",
    "* PCA: Performs the data reduction using principal component analysis  \n",
    "* PolynomialExpansion: Performs a polynomial expansion of a vector. For\n",
    "example, if you had a vector symbolically written as [x, y, z], the method\n",
    "would produce the following expansion: [x, x*x, y, x*y, y*y, z, x*z,\n",
    "y*z, z*z].  \n",
    "* QuantileDiscretizer: Similar to the Bucketizer method, but instead of\n",
    "passing the splits parameter, you pass the numBuckets one. The method then\n",
    "decides, by calculating approximate quantiles over your data, what the splits\n",
    "should be.  \n",
    "* RegexTokenizer: This is a string tokenizer using regular expressions.  \n",
    "* RFormula: For those of you who are avid R users, you can pass a formula\n",
    "such as vec ~ alpha * 3 + beta (assuming your DataFrame has the alpha\n",
    "and beta columns) and it will produce the vec column given the expression.  \n",
    "* SQLTransformer: Similar to the previous, but instead of R-like formulas,\n",
    "you can use SQL syntax. [The FROM statement should be selecting from __THIS__, indicating\n",
    "you are accessing the DataFrame. For example: SELECT alpha * 3 +\n",
    "beta AS vec FROM __THIS__.]  \n",
    "* StandardScaler: Standardizes the column to have a 0 mean and standard\n",
    "deviation equal to 1.  \n",
    "* StopWordsRemover: Removes stop words (such as 'the' or 'a') from a\n",
    "tokenized text.  \n",
    "* StringIndexer: Given a list of all the words in a column, this will produce\n",
    "a vector of indices  \n",
    "* Tokenizer: This is the default tokenizer that converts the string to lower case\n",
    "and then splits on space(s).  \n",
    "* VectorAssembler: This is a highly useful transformer that collates multiple\n",
    "numeric (vectors included) columns into a single column with a vector\n",
    "representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "[(12, 10, 3), (1, 4, 2)],\n",
    "['a', 'b', 'c'])\n",
    "\n",
    "ft.VectorAssembler(inputCols=['a', 'b', 'c'],\n",
    "outputCol='features')\\\n",
    ".transform(df) \\\n",
    ".select('features')\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VectorIndexer: This is a method for indexing categorical columns into a\n",
    "vector of indices. It works in a column-by-column fashion, selecting distinct\n",
    "values from the column, sorting and returning an index of the value from\n",
    "the map instead of the original value.  \n",
    "* VectorSlicer: Works on a feature vector, either dense or sparse: given a list\n",
    "of indices, it extracts the values from the feature vector.  \n",
    "* Word2Vec: This method takes a sentence (string) as an input and transforms\n",
    "it into a map of {string, vector} format, a representation that is useful in\n",
    "natural language processing.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimators  \n",
    "Estimators can be thought of as statistical models that need to be estimated to make\n",
    "predictions or classify your observations.  \n",
    "\n",
    "If deriving from the abstract Estimator class, the new model has to implement the\n",
    ".fit(...) method that fits the model given the data found in a DataFrame and\n",
    "some default or user-specified parameters.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification  \n",
    "\n",
    "The ML package provides a data scientist with seven classification models to\n",
    "choose from. These range from the simplest ones (such as logistic regression) to\n",
    "more sophisticated ones. We will provide short descriptions of each of them in the\n",
    "following section:  \n",
    "* LogisticRegression: The benchmark model for classification. The logistic\n",
    "regression uses a logit function to calculate the probability of an observation\n",
    "belonging to a particular class. At the time of writing, the PySpark ML\n",
    "supports only binary classification problems.  \n",
    "\n",
    "* DecisionTreeClassifier: A classifier that builds a decision tree to predict a\n",
    "class for an observation. Specifying the maxDepth parameter limits the depth\n",
    "the tree grows, the minInstancePerNode determines the minimum number\n",
    "of observations in the tree node required to further split, the maxBins\n",
    "parameter specifies the maximum number of bins the continuous variables\n",
    "will be split into, and the impurity specifies the metric to measure and\n",
    "calculate the information gain from the split.  \n",
    "\n",
    "* GBTClassifier: A Gradient Boosted Trees model for classification. The\n",
    "model belongs to the family of ensemble models: models that combine\n",
    "multiple weak predictive models to form a strong one. At the moment,\n",
    "the GBTClassifier model supports binary labels, and continuous and\n",
    "categorical features.  \n",
    "\n",
    "* RandomForestClassifier: This model produces multiple decision trees\n",
    "(hence the name—forest) and uses the mode output of those decision trees to\n",
    "classify observations. The RandomForestClassifier supports both binary\n",
    "and multinomial labels.  \n",
    "\n",
    "* NaiveBayes: Based on the Bayes' theorem, this model uses conditional\n",
    "probability theory to classify observations. The NaiveBayes model in\n",
    "PySpark ML supports both binary and multinomial labels.  \n",
    "\n",
    "* MultilayerPerceptronClassifier: A classifier that mimics the nature of\n",
    "a human brain. Deeply rooted in the Artificial Neural Networks theory, the\n",
    "model is a black-box, that is, it is not easy to interpret the internal parameters\n",
    "of the model. The model consists, at a minimum, of three, fully connected\n",
    "layers (a parameter that needs to be specified when creating the model\n",
    "object) of artificial neurons: the input layer (that needs to be equal to the\n",
    "number of features in your dataset), a number of hidden layers (at least one),\n",
    "and an output layer with the number of neurons equal to the number of\n",
    "categories in your label. All the neurons in the input and hidden layers have\n",
    "a sigmoid activation function, whereas the activation function of the neurons\n",
    "in the output layer is softmax.  \n",
    "\n",
    "* OneVsRest: A reduction of a multiclass classification to a binary one. For\n",
    "example, in the case of a multinomial label, the model can train multiple\n",
    "binary logistic regression models. For example, if label == 2, the model\n",
    "will build a logistic regression where it will convert the label == 2 to 1\n",
    "(all remaining label values would be set to 0) and then train a binary\n",
    "model. All the models are then scored and the model with the highest\n",
    "probability wins.  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression  \n",
    "\n",
    "There are seven models available for regression tasks in the PySpark ML package. As\n",
    "with classification, these range from some basic ones (such as the obligatory linear\n",
    "regression) to more complex ones:  \n",
    "* AFTSurvivalRegression: Fits an Accelerated Failure Time regression\n",
    "model. It is a parametric model that assumes that a marginal effect of one of\n",
    "the features accelerates or decelerates a life expectancy (or process failure).\n",
    "It is highly applicable for the processes with well-defined stages.  \n",
    "\n",
    "* DecisionTreeRegressor: Similar to the model for classification with\n",
    "an obvious distinction that the label is continuous instead of binary\n",
    "(or multinomial).  \n",
    "\n",
    "* GBTRegressor: As with the DecisionTreeRegressor, the difference is the\n",
    "data type of the label.  \n",
    "\n",
    "* GeneralizedLinearRegression: A family of linear models with differing\n",
    "kernel functions (link functions). In contrast to the linear regression that\n",
    "assumes normality of error terms, the GLM allows the label to have different\n",
    "error term distributions: the GeneralizedLinearRegression model from\n",
    "the PySpark ML package supports gaussian, binomial, gamma, and poisson\n",
    "families of error distributions with a host of different link functions.  \n",
    "\n",
    "* IsotonicRegression: A type of regression that fits a free-form, nondecreasing\n",
    "line to your data. It is useful to fit the datasets with ordered and\n",
    "increasing observations.  \n",
    "\n",
    "* LinearRegression: The most simple of regression models, it assumes a\n",
    "linear relationship between features and a continuous label, and normality\n",
    "of error terms.  \n",
    "\n",
    "* RandomForestRegressor: Similar to either DecisionTreeRegressor or\n",
    "GBTRegressor, the RandomForestRegressor fits a continuous label instead\n",
    "of a discrete one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering  \n",
    "\n",
    "Clustering is a family of unsupervised models that are used to find underlying\n",
    "patterns in your data. The PySpark ML package provides the four most popular\n",
    "models at the moment:  \n",
    "* BisectingKMeans: A combination of the k-means clustering method and\n",
    "hierarchical clustering. The algorithm begins with all observations in\n",
    "a single cluster and iteratively splits the data into k clusters.  \n",
    "\n",
    "* KMeans: This is the famous k-mean algorithm that separates data into\n",
    "k clusters, iteratively searching for centroids that minimize the sum of\n",
    "square distances between each observation and the centroid of the cluster\n",
    "it belongs to.  \n",
    "\n",
    "* GaussianMixture: This method uses k Gaussian distributions with unknown\n",
    "parameters to dissect the dataset. Using the Expectation-Maximization\n",
    "algorithm, the parameters for the Gaussians are found by maximizing the\n",
    "log-likelihood function. [Beware that for datasets with many features this model might\n",
    "perform poorly due to the curse of dimensionality and numerical\n",
    "issues with Gaussian distributions.]    \n",
    "\n",
    "* LDA: This model is used for topic modeling in natural language processing\n",
    "applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline  \n",
    "\n",
    "A Pipeline in PySpark ML is a concept of an end-to-end transformation-estimation\n",
    "process (with distinct stages) that ingests some raw data (in a DataFrame form),\n",
    "performs the necessary data carpentry (transformations), and finally estimates a\n",
    "statistical model (estimator).  [A Pipeline can be purely transformative, that is, consisting of\n",
    "Transformers only.]\n",
    "\n",
    "A Pipeline can be thought of as a chain of multiple discrete stages. When a\n",
    ".fit(...) method is executed on a Pipeline object, all the stages are executed in\n",
    "the order they were specified in the stages parameter; the stages parameter is a\n",
    "list of Transformer and Estimator objects. The .fit(...) method of the Pipeline\n",
    "object executes the .transform(...) method for the Transformers and the\n",
    ".fit(...) method for the Estimators.  \n",
    "Normally, the output of a preceding stage becomes the input for the following\n",
    "stage: when deriving from either the Transformer or Estimator abstract classes,\n",
    "one needs to implement the .getOutputCol() method that returns the value of the\n",
    "outputCol parameter specified when creating an object.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the chances of infant survival with ML  \n",
    "\n",
    "In this section, we will, once again, attempt to predict the chances of the survival of\n",
    "an infant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Loading the data**  \n",
    "\n",
    "First, we load the data with the help of the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ  \n",
    "\n",
    "labels = [\n",
    "    ('INFANT_ALIVE_AT_REPORT', typ.IntegerType()),\n",
    "    ('BIRTH_PLACE', typ.StringType()),\n",
    "    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
    "    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
    "    ('CIG_BEFORE', typ.IntegerType()),\n",
    "    ('CIG_1_TRI', typ.IntegerType()),\n",
    "    ('CIG_2_TRI', typ.IntegerType()),\n",
    "    ('CIG_3_TRI', typ.IntegerType()),\n",
    "    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
    "    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
    "    ('DIABETES_PRE', typ.IntegerType()),\n",
    "    ('DIABETES_GEST', typ.IntegerType()),\n",
    "    ('HYP_TENS_PRE', typ.IntegerType()),\n",
    "    ('HYP_TENS_GEST', typ.IntegerType()),\n",
    "    ('PREV_BIRTH_PRETERM', typ.IntegerType())\n",
    "]\n",
    "\n",
    "schema = typ.StructType([\n",
    "    typ.StructField(e[0], e[1], False) for e in labels\n",
    "])  \n",
    "\n",
    "\n",
    "births = spark.read.csv('datasets/births_transformed.csv.gz', \n",
    "                        header=True, \n",
    "                        schema=schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the schema of the DataFrame; our severely limited dataset now only\n",
    "has 17 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating transformers  \n",
    "\n",
    "Before we can use the dataset to estimate a model, we need to do some\n",
    "transformations. Since statistical models can only operate on numeric data,\n",
    "we will have to encode the BIRTH_PLACE variable.  \n",
    "\n",
    "Before we do any of this, since we will use a number of different feature\n",
    "transformations later in this chapter, let's import them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode the BIRTH_PLACE column, we will use the OneHotEncoder method.\n",
    "However, the method cannot accept StringType columns; it can only deal with\n",
    "numeric types so first we will cast the column to an IntegerType:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "births = births \\\n",
    ".withColumn('BIRTH_PLACE_INT', births['BIRTH_PLACE'] \\\n",
    ".cast(typ.IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this, we can now create our first Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ft.OneHotEncoder(\n",
    "inputCol='BIRTH_PLACE_INT',\n",
    "outputCol='BIRTH_PLACE_VEC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a single column with all the features collated together. We will use\n",
    "the VectorAssembler method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCreator = ft.VectorAssembler(\n",
    "inputCols=[col[0] for col in labels[2:]] + [encoder.getOutputCol()], outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputCols parameter passed to the VectorAssembler object is a list of all the\n",
    "columns to be combined together to form the outputCol—the 'features'. Note\n",
    "that we use the output of the encoder object (by calling the .getOutputCol()\n",
    "method), so we do not have to remember to change this parameter's value should\n",
    "we change the name of the output column in the encoder object at any point.  \n",
    "\n",
    "It's now time to create our first estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an estimator  \n",
    "\n",
    "In this example, we will (once again) use the logistic regression model. However,\n",
    "later in the chapter, we will showcase some more complex models from the\n",
    ".classification set of PySpark ML models, so we load the whole section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.classification as cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once loaded, let's create the model by using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(\n",
    "maxIter=10,\n",
    "regParam=0.01,\n",
    "labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would not have to specify the labelCol parameter if our target column had\n",
    "the name 'label'. Also, if the output of our featuresCreator was not called\n",
    "'features', we would have to specify the featuresCol by (most conveniently)\n",
    "calling the getOutputCol() method on the featuresCreator object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline  \n",
    "\n",
    "All that is left now is to create a Pipeline and fit the model. First, let's load the\n",
    "Pipeline from the ML package:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Pipeline is really easy. Here's how our pipeline should look like\n",
    "conceptually:  \n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting this structure into a Pipeline is a walk in the park:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    stages=[encoder,\n",
    "            featuresCreator,\n",
    "            logistic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Our pipeline is now created so we can (finally!) estimate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model  \n",
    "Before you fit the model, we need to split our dataset into training and testing\n",
    "datasets. Conveniently, the DataFrame API has the .randomSplit(...) method:  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_train, births_test = births.randomSplit([0.7, 0.3], seed=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter is a list of dataset proportions that should end up in, respectively,\n",
    "births_train and births_test subsets. The seed parameter provides a seed to the\n",
    "randomizer. \n",
    "You can also split the dataset into more than two subsets as long as the\n",
    "elements of the list sum up to 1, and you unpack the output into as many\n",
    "subsets.  \n",
    "\n",
    "For example, we could split the births dataset into three subsets like this:  \n",
    "***\n",
    "train, test, val = births.randomSplit([0.7, 0.2, 0.1], seed=666)  \n",
    "The preceding code would put a random 70% of the births dataset into the train object, 20% would go to the test, and the val DataFrame would hold the remaining 10%.  \n",
    "***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is about time to finally run our pipeline and estimate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(births_train)\n",
    "test_model = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .fit(...) method of the pipeline object takes our training dataset as an\n",
    "input. Under the hood, the births_train dataset is passed first to the encoder\n",
    "object. The DataFrame that is created at the encoder stage then gets passed to the\n",
    "featuresCreator that creates the 'features' column. Finally, the output from this\n",
    "stage is passed to the logistic object that estimates the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .fit(...) method of the pipeline object takes our training dataset as an\n",
    "input. Under the hood, the births_train dataset is passed first to the encoder\n",
    "object. The DataFrame that is created at the encoder stage then gets passed to the\n",
    "featuresCreator that creates the 'features' column. Finally, the output from this\n",
    "stage is passed to the logistic object that estimates the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get all the columns from the Transfomers and Estimators. The\n",
    "logistic regression model outputs several columns: the rawPrediction is the value\n",
    "of the linear combination of features and the β coefficients, the probability is the\n",
    "calculated probability for each of the classes, and finally, the prediction is our final\n",
    "class assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the performance of the model  \n",
    "\n",
    "Obviously, we would like to now test how well our model did. PySpark exposes a\n",
    "number of evaluation methods for classification and regression in the .evaluation\n",
    "section of the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the BinaryClassficationEvaluator to test how well our model\n",
    "performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='probability',\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rawPredictionCol can either be the rawPrediction column produced by the\n",
    "estimator or the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our model performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluator.evaluate(test_model, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(test_model, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the ROC of 74% and area under PR of 71% shows a well-defined\n",
    "model, but nothing out of extraordinary; if we had other features, we could drive\n",
    "this up, but this is not the purpose of this chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model  \n",
    "PySpark allows you to save the Pipeline definition for later use. It not only saves\n",
    "the pipeline structure, but also all the definitions of all the Transformers and\n",
    "Estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ipelinePath = './infant_oneHotEncoder_Logistic_Pipeline'\n",
    "#pipeline.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can load it up later and use it straight away to .fit(...) and predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadedPipeline = Pipeline.load(pipelinePath)\n",
    "# loadedPipeline.fit(births_train).transform(births_test).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you, however, want to save the estimated model, you can also do that; instead of\n",
    "saving the Pipeline, you need to save the PipelineModel.  \n",
    "[Note, that not only the PipelineModel can be saved: virtually all\n",
    "the models that are returned by calling the .fit(...) method on an\n",
    "Estimator or Transformer can be saved and loaded back to be reused.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save your model, see the following the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelPath = 'C:/Users/balde/OneDrive/Bureau/TO DO/DE/Learning_PySpark/model'\n",
    "# model.write().overwrite().save(modelPath)\n",
    "\n",
    "# loadedPipelineModel = PipelineModel.load(modelPath)\n",
    "# test_reloadedModel = loadedPipelineModel.transform(births_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding script uses the .load(...) method, a class method of the\n",
    "PipelineModel class, to reload the estimated model. You can compare the result\n",
    "of test_reloadedModel.take(1) with the output of test_model.take(1) we\n",
    "presented earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter hyper-tuning  \n",
    "\n",
    "Rarely, our first model would be the best we can do. By simply looking at our\n",
    "metrics and accepting the model because it passed our pre-conceived performance\n",
    "thresholds is hardly a scientific method for finding the best model.  \n",
    "\n",
    "A concept of parameter hyper-tuning is to find the best parameters of the model: for\n",
    "example, the maximum number of iterations needed to properly estimate the logistic\n",
    "regression model or maximum depth of a decision tree.  \n",
    "\n",
    "In this section, we will explore two concepts that allow us to find the best parameters\n",
    "for our models: grid search and train-validation splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search  \n",
    "\n",
    "Grid search is an exhaustive algorithm that loops through the list of defined\n",
    "parameter values, estimates separate models, and chooses the best one given\n",
    "some evaluation metric.  \n",
    "\n",
    "A note of caution should be stated here: if you define too many parameters you\n",
    "want to optimize over, or too many values of these parameters, it might take a lot of\n",
    "time to select the best model as the number of models to estimate would grow very\n",
    "quickly as the number of parameters and parameter values grow.  \n",
    "\n",
    "For example, if you want to fine-tune two parameters with two parameter values,\n",
    "you would have to fit four models. Adding one more parameter with two values\n",
    "would require estimating eight models, whereas adding one more additional value\n",
    "to our two parameters (bringing it to three values for each) would require estimating\n",
    "nine models. As you can see, this can quickly get out of hand if you are not careful.  \n",
    "\n",
    "After this cautionary tale, let's get to fine-tuning our parameters space. First, we load\n",
    "the .tuning part of the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's specify our model and the list of parameters we want to loop through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "\n",
    "grid = tune.ParamGridBuilder(\n",
    "    ).addGrid(logistic.maxIter, \n",
    "              [2, 10, 50]\n",
    "    ).addGrid(logistic.regParam, \n",
    "                [0.01, 0.05, 0.3]\n",
    "    ).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify the model we want to optimize the parameters of. Next, we decide\n",
    "which parameters we will be optimizing, and what values for those parameters to\n",
    "test. We use the ParamGridBuilder() object from the .tuning subpackage, and\n",
    "keep adding the parameters to the grid with the .addGrid(...) method: the first\n",
    "parameter is the parameter object of the model we want to optimize (in our case,\n",
    "these are logistic.maxIter and logistic.regParam), and the second parameter\n",
    "is a list of values we want to loop through. Calling the .build() method on the\n",
    ".ParamGridBuilder builds the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need some way of comparing the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='INFANT_ALIVE_AT_REPORT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, once again, we'll use the BinaryClassificationEvaluator. It is time now to\n",
    "create the logic that will do the validation work for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(estimator=logistic, estimatorParamMaps=grid, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CrossValidator needs the estimator, the estimatorParamMaps, and the\n",
    "evaluator to do its job. The model loops through the grid of values, estimates\n",
    "the models, and compares their performance using the evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use the data straight away (as the births_train and births_test still\n",
    "have the BIRTHS_PLACE column not encoded) so we create a purely transforming\n",
    "Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[encoder ,featuresCreator])\n",
    "data_transformer = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this, we are ready to find the optimal combination of parameters for\n",
    "our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(data_transformer.transform(births_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cvModel will return the best model estimated. We can now use it to see if it\n",
    "performed better than our previous model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_transformer.transform(births_test)\n",
    "\n",
    "results = cvModel.transform(data_train)\n",
    "\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What parameters has the best model? The answer is a little bit convoluted but here's how you can extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    (\n",
    "        [\n",
    "            {key.name: paramValue} \n",
    "            for key, paramValue \n",
    "            in zip(\n",
    "                params.keys(), \n",
    "                params.values())\n",
    "        ], metric\n",
    "    ) \n",
    "    for params, metric \n",
    "    in zip(\n",
    "        cvModel.getEstimatorParamMaps(), \n",
    "        cvModel.avgMetrics\n",
    "    )\n",
    "]\n",
    "\n",
    "sorted(results, key=lambda el: el[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-validation splitting  \n",
    "\n",
    "The TrainValidationSplit model, to select the best model, performs a random\n",
    "split of the input dataset (the training dataset) into two subsets: smaller training\n",
    "and validation subsets. The split is only performed once.  \n",
    "\n",
    "In this example, we will also use the ChiSqSelector to select only the top five\n",
    "features, thus limiting the complexity of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ft.ChiSqSelector(\n",
    "    numTopFeatures=5,\n",
    "    featuresCol=featuresCreator.getOutputCol(),\n",
    "    outputCol='selectedFeatures',\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numTopFeatures specifies the number of features to return. We will put\n",
    "the selector after the featuresCreator, so we call the .getOutputCol() on the\n",
    "featuresCreator.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(\n",
    "    labelCol='INFANT_ALIVE_AT_REPORT',\n",
    "    featuresCol='selectedFeatures'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, selector])\n",
    "data_transformer = pipeline.fit(births_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TrainValidationSplit object gets created in the same fashion as the\n",
    "CrossValidator model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = tune.TrainValidationSplit(\n",
    "    estimator=logistic,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we fit our data to the model, and calculate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsModel = tvs.fit(data_transformer.transform(births_train))\n",
    "\n",
    "data_train = data_transformer.transform(births_test)  \n",
    "\n",
    "results = tvsModel.transform(data_train)  \n",
    "\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the model with less features certainly performed worse than the full model, but\n",
    "the difference was not that great. Ultimately, it is a performance trade-off between a\n",
    "more complex model and the less sophisticated one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other features of PySpark ML in action  \n",
    "\n",
    "At the beginning of this chapter, we described most of the features of the PySpark\n",
    "ML library. In this section, we will provide examples of how to use some of the\n",
    "Transformers and Estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction  \n",
    "\n",
    "We have used quite a few models from this submodule of PySpark. In this section,\n",
    "we'll show you how to use the most useful ones (in our opinion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP - related feature extractors  \n",
    "\n",
    "As described earlier, the NGram model takes a list of tokenized text and produces\n",
    "pairs (or n-grams) of words.  \n",
    "\n",
    "In this example, we will take an excerpt from PySpark's documentation and present\n",
    "how to clean up the text before passing it to the NGram model. Here's how our dataset\n",
    "looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = spark.createDataFrame([\n",
    "    ['''Machine learning can be applied to a wide variety \n",
    "        of data types, such as vectors, text, images, and \n",
    "        structured data. This API adopts the DataFrame from \n",
    "        Spark SQL in order to support a variety of data types.'''],\n",
    "    ['''DataFrame supports many basic and structured types; \n",
    "        see the Spark SQL datatype reference for a list of \n",
    "        supported types. In addition to the types listed in \n",
    "        the Spark SQL guide, DataFrame can use ML Vector types.'''],\n",
    "    ['''A DataFrame can be created either implicitly or \n",
    "        explicitly from a regular RDD. See the code examples \n",
    "        below and the Spark SQL programming guide for examples.'''],\n",
    "    ['''Columns in a DataFrame are named. The code examples \n",
    "        below use names such as \"text,\" \"features,\" and \"label.\"''']\n",
    "], ['input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in our single-column DataFrame is just a bunch of text. First, we need\n",
    "to tokenize this text. To do so we will use the RegexTokenizer instead of just the\n",
    "Tokenizer as we can specify the pattern(s) we want the text to be broken at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ft.RegexTokenizer(\n",
    "    inputCol='input',\n",
    "    outputCol='input_arr',\n",
    "    pattern='\\s+|[,.\\\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern here splits the text on any number of spaces, but also removes commas,\n",
    "full stops, backslashes, and quotation marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the tokenizer looks similar to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer.transform(text_data).select('input_arr') \n",
    "\n",
    "tok.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the RegexTokenizer not only splits the sentences in to words, but\n",
    "also normalizes the text so each word is in small-caps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is still plenty of junk in our text: words such as be, a, or to normally\n",
    "provide us with nothing useful when analyzing a text. Thus, we will remove these\n",
    "so called stopwords using nothing else other than the StopWordsRemover(...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ft.StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='input_stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the method looks as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.transform(tok).select('input_stop').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only have the useful words. So, let's build our NGram model and the\n",
    "Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = ft.NGram(n=2, inputCol=stopwords.getOutputCol(), outputCol=\"nGrams\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords, ngram])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the pipeline, we follow in a very similar fashion as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ngram = pipeline.fit(text_data).transform(text_data)\n",
    "\n",
    "data_ngram.select('nGrams').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. We have got our n-grams and we can now use them in further NLP\n",
    "processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretizing continuous variables  \n",
    "\n",
    "Ever so often, we deal with a continuous feature that is highly non-linear and really\n",
    "hard to fit in our model with only one coefficient.  \n",
    "\n",
    "In such a situation, it might be hard to explain the relationship between such a\n",
    "feature and the target with just one coefficient. Sometimes, it is useful to band the\n",
    "values into discrete buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create some fake data with the help of the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.arange(0, 100)\n",
    "x = x / 100.0 * np.pi * 4\n",
    "y = x * np.sin(x / 1.764) + 20.1234 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a DataFrame by using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = typ.StructType([typ.StructField('continuous_var',typ.DoubleType(),False)])\n",
    "\n",
    "data = spark.createDataFrame([[float(e), ] for e in y], schema=schema)\n",
    "data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the QuantileDiscretizer model to split our continuous variable\n",
    "into five buckets (the numBuckets parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer = ft.QuantileDiscretizer(numBuckets=5, inputCol='continuous_var', outputCol='discretized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we have got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_discretized = discretizer.fit(data).transform(data)\n",
    "data_discretized.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_discretized \\\n",
    "    .groupby('discretized')\\\n",
    "    .mean('continuous_var')\\\n",
    "    .sort('discretized')\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now treat this variable as categorical and use the OneHotEncoder to encode it\n",
    "for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing continuous variables  \n",
    "\n",
    "Standardizing continuous variables helps not only in better understanding the\n",
    "relationships between the features (as interpreting the coefficients becomes easier),\n",
    "but it also aids computational efficiency and protects from running into some\n",
    "numerical traps. Here's how you do it with PySpark ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a vector representation of our continuous variable (as it is\n",
    "only a single float):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = ft.VectorAssembler(inputCols=['continuous_var'], outputCol= 'continuous_vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build our normalizer and the pipeline. By setting the withMean and\n",
    "withStd to True, the method will remove the mean and scale the variance to be\n",
    "of unit length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = ft.StandardScaler(inputCol=vectorizer.getOutputCol(), outputCol='normalized', withMean=True, withStd=True)\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorizer, normalizer])\n",
    "\n",
    "data_standardized = pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standardized.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification  \n",
    "\n",
    "So far we have only used the LogisticRegression model from PySpark ML. In this\n",
    "section, we will use the RandomForestClassfier to, once again, model the chances\n",
    "of survival for an infant.  \n",
    "\n",
    "Before we can do that, though, we need to cast the label feature to DoubleType:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "births = births.withColumn(\n",
    "    'INFANT_ALIVE_AT_REPORT',\n",
    "    func.col('INFANT_ALIVE_AT_REPORT').cast(typ.DoubleType())\n",
    ")\n",
    "births_train, births_test = births.randomSplit([0.7, 0.3], seed=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the label converted to double, we are ready to build our model.\n",
    "We progress in a similar fashion as before with the distinction that we will reuse the\n",
    "encoder and featureCreator from earlier in the chapter. The numTrees parameter\n",
    "specifies how many decision trees should be in our random forest, and the maxDepth\n",
    "parameter limits the depth of the trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = cl.RandomForestClassifier(numTrees=5, maxDepth=5, labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, classifier])\n",
    "\n",
    "model = pipeline.fit(births_train)\n",
    "test = model.transform(births_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how the RandomForestClassifier model performs compared to the\n",
    "LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "\n",
    "print(evaluator.evaluate(test, {evaluator.metricName: \"areaUnderROC\"}))\n",
    "print(evaluator.evaluate(test, {evaluator.metricName: \"areaUnderPR\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, as you can see, the results are better than the logistic regression model by\n",
    "roughly 3 percentage points. Let's test how well would a model with one tree do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = cl.DecisionTreeClassifier(maxDepth=5, labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, classifier])\n",
    "\n",
    "model = pipeline.fit(births_train)\n",
    "test = model.transform(births_test)\n",
    "\n",
    "evaluator = ev.BinaryClassificationEvaluator(labelCol='INFANT_ALIVE_AT_REPORT')\n",
    "\n",
    "print(evaluator.evaluate(test, {evaluator.metricName: \"areaUnderROC\"}))\n",
    "print(evaluator.evaluate(test, {evaluator.metricName: \"areaUnderPR\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all! It actually performed better than the random forest model in terms of\n",
    "the precision-recall relationship and only slightly worse in terms of the area under\n",
    "the ROC. We just might have found a winner!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering  \n",
    "Clustering is another big part of machine learning: quite often, in the real world,\n",
    "we do not have the luxury of having the target feature, so we need to revert to an\n",
    "unsupervised learning paradigm, where we try to uncover patterns in the data.  \n",
    "\n",
    "#### Finding clusters in the births dataset  \n",
    "\n",
    "In this example, we will use the k-means model to find similarities in the births data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.clustering as clus\n",
    "\n",
    "kmeans = clus.KMeans(k = 5, featuresCol='features')\n",
    "\n",
    "pipeline = Pipeline(stages=[encoder, featuresCreator, kmeans])\n",
    "\n",
    "model = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having estimated the model, let's see if we can find some differences between\n",
    "clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.transform(births_test)\n",
    "\n",
    "test.groupBy('prediction').agg({\n",
    "        '*': 'count', \n",
    "        'MOTHER_HEIGHT_IN': 'avg'\n",
    "    }).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the MOTHER_HEIGHT_IN is significantly different in cluster 1. Going through the\n",
    "results (which we will not do here for obvious reasons) would most likely uncover\n",
    "more differences and allow us to understand the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic mining  \n",
    "\n",
    "Clustering models are not limited to numeric data only. In the field of NLP, problems\n",
    "such as topic extraction rely on clustering to detect documents with similar topics.\n",
    "We will go through such an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create our dataset. The data is formed from randomly selected paragraphs\n",
    "found on the Internet: three of them deal with topics of nature and national parks,\n",
    "the remaining three cover technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = spark.createDataFrame([\n",
    "    ['''To make a computer do anything, you have to write a \n",
    "    computer program. To write a computer program, you have \n",
    "    to tell the computer, step by step, exactly what you want \n",
    "    it to do. The computer then \"executes\" the program, \n",
    "    following each step mechanically, to accomplish the end \n",
    "    goal. When you are telling the computer what to do, you \n",
    "    also get to choose how it's going to do it. That's where \n",
    "    computer algorithms come in. The algorithm is the basic \n",
    "    technique used to get the job done. Let's follow an \n",
    "    example to help get an understanding of the algorithm \n",
    "    concept.'''],\n",
    "    ['''Laptop computers use batteries to run while not \n",
    "    connected to mains. When we overcharge or overheat \n",
    "    lithium ion batteries, the materials inside start to \n",
    "    break down and produce bubbles of oxygen, carbon dioxide, \n",
    "    and other gases. Pressure builds up, and the hot battery \n",
    "    swells from a rectangle into a pillow shape. Sometimes \n",
    "    the phone involved will operate afterwards. Other times \n",
    "    it will die. And occasionally—kapow! To see what's \n",
    "    happening inside the battery when it swells, the CLS team \n",
    "    used an x-ray technology called computed tomography.'''],\n",
    "    ['''This technology describes a technique where touch \n",
    "    sensors can be placed around any side of a device \n",
    "    allowing for new input sources. The patent also notes \n",
    "    that physical buttons (such as the volume controls) could \n",
    "    be replaced by these embedded touch sensors. In essence \n",
    "    Apple could drop the current buttons and move towards \n",
    "    touch-enabled areas on the device for the existing UI. It \n",
    "    could also open up areas for new UI paradigms, such as \n",
    "    using the back of the smartphone for quick scrolling or \n",
    "    page turning.'''],\n",
    "    ['''The National Park Service is a proud protector of \n",
    "    America’s lands. Preserving our land not only safeguards \n",
    "    the natural environment, but it also protects the \n",
    "    stories, cultures, and histories of our ancestors. As we \n",
    "    face the increasingly dire consequences of climate \n",
    "    change, it is imperative that we continue to expand \n",
    "    America’s protected lands under the oversight of the \n",
    "    National Park Service. Doing so combats climate change \n",
    "    and allows all American’s to visit, explore, and learn \n",
    "    from these treasured places for generations to come. It \n",
    "    is critical that President Obama acts swiftly to preserve \n",
    "    land that is at risk of external threats before the end \n",
    "    of his term as it has become blatantly clear that the \n",
    "    next administration will not hold the same value for our \n",
    "    environment over the next four years.'''],\n",
    "    ['''The National Park Foundation, the official charitable \n",
    "    partner of the National Park Service, enriches America’s \n",
    "    national parks and programs through the support of \n",
    "    private citizens, park lovers, stewards of nature, \n",
    "    history enthusiasts, and wilderness adventurers. \n",
    "    Chartered by Congress in 1967, the Foundation grew out of \n",
    "    a legacy of park protection that began over a century \n",
    "    ago, when ordinary citizens took action to establish and \n",
    "    protect our national parks. Today, the National Park \n",
    "    Foundation carries on the tradition of early park \n",
    "    advocates, big thinkers, doers and dreamers—from John \n",
    "    Muir and Ansel Adams to President Theodore Roosevelt.'''],\n",
    "    ['''Australia has over 500 national parks. Over 28 \n",
    "    million hectares of land is designated as national \n",
    "    parkland, accounting for almost four per cent of \n",
    "    Australia's land areas. In addition, a further six per \n",
    "    cent of Australia is protected and includes state \n",
    "    forests, nature parks and conservation reserves.National \n",
    "    parks are usually large areas of land that are protected \n",
    "    because they have unspoilt landscapes and a diverse \n",
    "    number of native plants and animals. This means that \n",
    "    commercial activities such as farming are prohibited and \n",
    "    human activity is strictly monitored.''']\n",
    "], ['documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will once again use the RegexTokenizer and the StopWordsRemover models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ft.RegexTokenizer(\n",
    "    inputCol='documents', \n",
    "    outputCol='input_arr', \n",
    "    pattern='\\s+|[,.\\\"]')\n",
    "\n",
    "stopwords = ft.StopWordsRemover(\n",
    "    inputCol=tokenizer.getOutputCol(), \n",
    "    outputCol='input_stop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next in our pipeline is the CountVectorizer: a model that counts words in a\n",
    "document and returns a vector of counts. The length of the vector is equal to the\n",
    "total number of distinct words in all the documents, which can be seen in the\n",
    "following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stringIndexer = ft.CountVectorizer(\n",
    "    inputCol=stopwords.getOutputCol(), \n",
    "    outputCol=\"input_indexed\")\n",
    "\n",
    "tokenized = stopwords.transform(tokenizer.transform(text_data))\n",
    "    \n",
    "stringIndexer.fit(tokenized).transform(tokenized).select('input_indexed').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are 257 distinct words in the text, and each document is now\n",
    "represented by a count of each word occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to start predicting the topics. For that purpose we will use the LDA\n",
    "model—the Latent Dirichlet Allocation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = clus.LDA(k=2, optimizer='online', featuresCol=stringIndexer.getOutputCol())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k parameter specifies how many topics we expect to see, the optimizer\n",
    "parameter can be either 'online' or 'em' (the latter standing for the Expectation\n",
    "Maximization algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting these puzzles together results in, so far, the longest of our pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "        tokenizer, \n",
    "        stopwords,\n",
    "        stringIndexer, \n",
    "        clustering]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have we properly uncovered the topics? Well, let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pipeline \\\n",
    "    .fit(text_data) \\\n",
    "    .transform(text_data)\n",
    "\n",
    "topics.select('topicDistribution').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our method discovered all the topics properly! Do not get used to seeing\n",
    "such good results though: sadly, real world data is seldom that kind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression  \n",
    "\n",
    "We could not finish a chapter on a machine learning library without building a\n",
    "regression model.  \n",
    "\n",
    "In this section, we will try to predict the MOTHER_WEIGHT_GAIN given some of the\n",
    "features described here; these are contained in the features listed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['MOTHER_AGE_YEARS','MOTHER_HEIGHT_IN',\n",
    "            'MOTHER_PRE_WEIGHT','DIABETES_PRE',\n",
    "            'DIABETES_GEST','HYP_TENS_PRE', \n",
    "            'HYP_TENS_GEST', 'PREV_BIRTH_PRETERM',\n",
    "            'CIG_BEFORE','CIG_1_TRI', 'CIG_2_TRI', \n",
    "            'CIG_3_TRI'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, since all the features are numeric, we will collate them together and use the\n",
    "ChiSqSelector to select only the top six most important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCreator = ft.VectorAssembler(\n",
    "    inputCols=[col for col in features[1:]], \n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "selector = ft.ChiSqSelector(\n",
    "    numTopFeatures=6, \n",
    "    outputCol=\"selectedFeatures\", \n",
    "    labelCol='MOTHER_WEIGHT_GAIN'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict the weight gain, we will use the gradient boosted trees regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.regression as reg\n",
    "\n",
    "regressor = reg.GBTRegressor(\n",
    "    maxIter=15, \n",
    "    maxDepth=3,\n",
    "    labelCol='MOTHER_WEIGHT_GAIN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, again, we put it all together into a Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "        featuresCreator, \n",
    "        selector,\n",
    "        regressor])\n",
    "\n",
    "weightGain = pipeline.fit(births_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having created the weightGain model, let's see if it performs well on our\n",
    "testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.RegressionEvaluator(\n",
    "    predictionCol=\"prediction\", \n",
    "    labelCol='MOTHER_WEIGHT_GAIN')\n",
    "\n",
    "print(evaluator.evaluate(\n",
    "    weightGain.transform(births_test), \n",
    "    {evaluator.metricName: 'r2'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, the model is no better than a flip of a coin. It looks that without additional\n",
    "independent features that are better correlated with the MOTHER_WEIGHT_GAIN label,\n",
    "we will not be able to explain its variance sufficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this chapter, we went into details of how to use PySpark ML: the official main\n",
    "machine learning library for PySpark. We explained what the Transformer and\n",
    "Estimator are, and showed their role in another concept introduced in the ML\n",
    "library: the Pipeline. Subsequently, we also presented how to use some of the\n",
    "methods to fine-tune the hyper parameters of models. Finally, we gave some\n",
    "examples of how to use some of the feature extractors and models from the library.  \n",
    "\n",
    "In the next chapter, we will delve into graph theory and GraphFrames that help in\n",
    "tackling machine learning problems better represented as graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_envi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
