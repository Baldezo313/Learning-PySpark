{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing MLlib  \n",
    "\n",
    "In the previous chapter, we learned how to prepare the data for modeling. In this chapter, we will actually use some of that learning to build a classification model using the MLlib package of PySpark.  \n",
    "\n",
    "MLlib stands for Machine Learning Library. Even though MLlib is now in a maintenance mode, that is, it is not actively being developed (and will most likely be deprecated later), it is warranted that we cover at least some of the features of the library. In addition, MLlib is currently the only library that supports training models for streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.types as typ\n",
    "import findspark\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "from bokeh.models import ColumnDataSource, FactorRange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(\"C:/Program Files/spark-3.5.4-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jre1.8.0_431\" \n",
    "os.environ[\"SPARK_HOME\"] = \"C:/Program Files/spark-3.5.4-bin-hadoop3\" \n",
    "os.environ['HADOOP_HOME '] = 'C:/Program Files/hadoop-3.4.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # Accès au SparkContext à partir de SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Accéder au SparkContext à partir de SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, you will learn how to do the following:   \n",
    "• Prepare the data for modeling with MLlib  \n",
    "• Perform statistical testing  \n",
    "• Predict survival chances of infants using logistic regression  \n",
    "• Select the most predictable features and train a random forest model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the high level, MLlib exposes three core machine learning functionalities:  \n",
    "• **Data preparation:** Feature extraction, transformation, selection, hashing of\n",
    "categorical features, and some natural language processing methods  \n",
    "• **Machine learning algorithms:** Some popular and advanced regression,\n",
    "classification, and clustering algorithms are implemented  \n",
    "• **Utilities:** Statistical methods such as descriptive statistics, chi-square testing,\n",
    "linear algebra (sparse and dense matrices and vectors), and model evaluation\n",
    "methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the palette of available functionalities allows you to perform almost\n",
    "all of the fundamental data science tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will build two classification models: a linear regression and\n",
    "a random forest. We will use a portion of the US 2014 and 2015 birth data we\n",
    "downloaded from http://www.cdc.gov/nchs/data_access/vitalstatsonline.\n",
    "htm; from the total of 300 variables we selected 85 features that we will use to build\n",
    "our models. Also, out of the total of almost 7.99 million records, we selected a\n",
    "balanced sample of 45,429 records: 22,080 records where infants were reported dead\n",
    "and 23,349 records with infants alive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and transforming the data  \n",
    "Even though MLlib is designed with RDDs and DStreams in focus, for ease of\n",
    "transforming the data we will read the data and convert it to a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we first specify the schema of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "\n",
    "labels = [\n",
    "    ('INFANT_ALIVE_AT_REPORT', typ.StringType()),\n",
    "    ('BIRTH_YEAR', typ.IntegerType()),\n",
    "    ('BIRTH_MONTH', typ.IntegerType()),\n",
    "    ('BIRTH_PLACE', typ.StringType()),\n",
    "    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
    "    ('MOTHER_RACE_6CODE', typ.StringType()),\n",
    "    ('MOTHER_EDUCATION', typ.StringType()),\n",
    "    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
    "    ('FATHER_EDUCATION', typ.StringType()),\n",
    "    ('MONTH_PRECARE_RECODE', typ.StringType()),\n",
    "    ('CIG_BEFORE', typ.IntegerType()),\n",
    "    ('CIG_1_TRI', typ.IntegerType()),\n",
    "    ('CIG_2_TRI', typ.IntegerType()),\n",
    "    ('CIG_3_TRI', typ.IntegerType()),\n",
    "    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
    "    ('MOTHER_BMI_RECODE', typ.IntegerType()),\n",
    "    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
    "    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
    "    ('DIABETES_PRE', typ.StringType()),\n",
    "    ('DIABETES_GEST', typ.StringType()),\n",
    "    ('HYP_TENS_PRE', typ.StringType()),\n",
    "    ('HYP_TENS_GEST', typ.StringType()),\n",
    "    ('PREV_BIRTH_PRETERM', typ.StringType()),\n",
    "    ('NO_RISK', typ.StringType()),\n",
    "    ('NO_INFECTIONS_REPORTED', typ.StringType()),\n",
    "    ('LABOR_IND', typ.StringType()),\n",
    "    ('LABOR_AUGM', typ.StringType()),\n",
    "    ('STEROIDS', typ.StringType()),\n",
    "    ('ANTIBIOTICS', typ.StringType()),\n",
    "    ('ANESTHESIA', typ.StringType()),\n",
    "    ('DELIV_METHOD_RECODE_COMB', typ.StringType()),\n",
    "    ('ATTENDANT_BIRTH', typ.StringType()),\n",
    "    ('APGAR_5', typ.IntegerType()),\n",
    "    ('APGAR_5_RECODE', typ.StringType()),\n",
    "    ('APGAR_10', typ.IntegerType()),\n",
    "    ('APGAR_10_RECODE', typ.StringType()),\n",
    "    ('INFANT_SEX', typ.StringType()),\n",
    "    ('OBSTETRIC_GESTATION_WEEKS', typ.IntegerType()),\n",
    "    ('INFANT_WEIGHT_GRAMS', typ.IntegerType()),\n",
    "    ('INFANT_ASSIST_VENTI', typ.StringType()),\n",
    "    ('INFANT_ASSIST_VENTI_6HRS', typ.StringType()),\n",
    "    ('INFANT_NICU_ADMISSION', typ.StringType()),\n",
    "    ('INFANT_SURFACANT', typ.StringType()),\n",
    "    ('INFANT_ANTIBIOTICS', typ.StringType()),\n",
    "    ('INFANT_SEIZURES', typ.StringType()),\n",
    "    ('INFANT_NO_ABNORMALITIES', typ.StringType()),\n",
    "    ('INFANT_ANCEPHALY', typ.StringType()),\n",
    "    ('INFANT_MENINGOMYELOCELE', typ.StringType()),\n",
    "    ('INFANT_LIMB_REDUCTION', typ.StringType()),\n",
    "    ('INFANT_DOWN_SYNDROME', typ.StringType()),\n",
    "    ('INFANT_SUSPECTED_CHROMOSOMAL_DISORDER', typ.StringType()),\n",
    "    ('INFANT_NO_CONGENITAL_ANOMALIES_CHECKED', typ.StringType()),\n",
    "    ('INFANT_BREASTFED', typ.StringType())\n",
    "]\n",
    "\n",
    "schema = typ.StructType([\n",
    "    typ.StructField(e[0], e[1], False) for e in labels\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the data. The .read.csv(...) method can read either uncompressed\n",
    "or (as in our case) GZipped comma-separated values. The header parameter set\n",
    "to True indicates that the first row contains the header, and we use the schema to\n",
    "specify the correct data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "births = spark.read.csv('datasets/births_train.csv.gz', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of features in our dataset that are strings. These are mostly\n",
    "categorical variables that we need to somehow convert to a numeric form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[You can glimpse over the original file schema specification here:\n",
    "ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/DVS/natality/UserGuide2015.pdf.]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will first specify our recode dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recode_dictionary = {\n",
    "'YNU': {\n",
    "'Y': 1,\n",
    "'N': 0,\n",
    "'U': 0\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this chapter is to predict whether the 'INFANT_ALIVE_AT_REPORT' is\n",
    "either 1 or 0. Thus, we will drop all of the features that relate to the infant and will\n",
    "try to predict the infant's chances of surviving only based on the features related to\n",
    "its mother, father, and the place of birth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    'BIRTH_PLACE', \n",
    "    'MOTHER_AGE_YEARS', \n",
    "    'FATHER_COMBINED_AGE', \n",
    "    'CIG_BEFORE', \n",
    "    'CIG_1_TRI', \n",
    "    'CIG_2_TRI', \n",
    "    'CIG_3_TRI', \n",
    "    'MOTHER_HEIGHT_IN', \n",
    "    'MOTHER_PRE_WEIGHT', \n",
    "    'MOTHER_DELIVERY_WEIGHT', \n",
    "    'MOTHER_WEIGHT_GAIN', \n",
    "    'DIABETES_PRE', \n",
    "    'DIABETES_GEST', \n",
    "    'HYP_TENS_PRE', \n",
    "    'HYP_TENS_GEST', \n",
    "    'PREV_BIRTH_PRETERM'\n",
    "]\n",
    "\n",
    "births_trimmed = births.select(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In our dataset, there are plenty of features with Yes/No/Unknown values; we will\n",
    "only code Yes to 1; everything else will be set to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a small problem with how the number of cigarettes smoked by the\n",
    "mother was coded: as 0 means the mother smoked no cigarettes before or during the\n",
    "pregnancy, between 1-97 states the actual number of cigarette smoked, 98 indicates\n",
    "either 98 or more, whereas 99 identifies the unknown; we will assume the unknown\n",
    "is 0 and recode accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So next we will specify our recoding methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "def recode(col, key):\n",
    "    return recode_dictionary[key][col]\n",
    "\n",
    "def correct_cig(feat):\n",
    "    return func.when(func.col(feat) != 99, func.col(feat)).otherwise(0)\n",
    "\n",
    "rec_integer = func.udf(recode, typ.IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recode method looks up the correct key from the recode_dictionary (given\n",
    "the key) and returns the corrected value. The correct_cig method checks when the\n",
    "value of the feature feat is not equal to 99 and (for that situation) returns the value\n",
    "of the feature; if the value is equal to 99, we get 0 otherwise.  \n",
    "\n",
    "We cannot use the recode function directly on a DataFrame; it needs to be converted\n",
    "to a UDF that Spark will understand. The rec_integer is such a function: by passing\n",
    "our specified recode function and specifying the return value data type, we can use\n",
    "it then to encode our Yes/No/Unknown features.  \n",
    "\n",
    "So, let's get to it. First, we'll correct the features related to the number of cigarettes\n",
    "smoked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_transformed = births_trimmed \\\n",
    "    .withColumn('CIG_BEFORE', correct_cig('CIG_BEFORE'))\\\n",
    "    .withColumn('CIG_1_TRI', correct_cig('CIG_1_TRI'))\\\n",
    "    .withColumn('CIG_2_TRI', correct_cig('CIG_2_TRI'))\\\n",
    "    .withColumn('CIG_3_TRI', correct_cig('CIG_3_TRI'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .withColumn(...) method takes the name of the column as its first parameter\n",
    "and the transformation as the second one. In the previous cases, we do not create\n",
    "new columns, but reuse the same ones instead.  \n",
    "\n",
    "Now we will focus on correcting the Yes/No/Unknown features. First, we will\n",
    "figure out which these are with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [(col.name, col.dataType) for col in births_trimmed.schema]\n",
    "\n",
    "YNU_cols = []\n",
    "\n",
    "for i, s in enumerate(cols):\n",
    "    if s[1] == typ.StringType():\n",
    "        dis = births.select(s[0]).distinct().rdd.map(lambda row: row[0]).collect()\n",
    "        \n",
    "        if 'Y' in dis:\n",
    "            YNU_cols.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we created a list of tuples (cols) that hold column names and corresponding\n",
    "data types. Next, we loop through all of these and calculate distinct values of all\n",
    "string columns; if a 'Y' is within the returned list, we append the column name to\n",
    "the YNU_cols list.  \n",
    "\n",
    "DataFrames can transform the features in bulk while selecting features. To present\n",
    "the idea, consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(INFANT_NICU_ADMISSION='Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
       " Row(INFANT_NICU_ADMISSION='Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
       " Row(INFANT_NICU_ADMISSION='U', INFANT_NICU_ADMISSION_RECODE=0),\n",
       " Row(INFANT_NICU_ADMISSION='N', INFANT_NICU_ADMISSION_RECODE=0),\n",
       " Row(INFANT_NICU_ADMISSION='U', INFANT_NICU_ADMISSION_RECODE=0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "births.select([\n",
    "        'INFANT_NICU_ADMISSION', \n",
    "        rec_integer(\n",
    "            'INFANT_NICU_ADMISSION', func.lit('YNU')\n",
    "        ) \\\n",
    "        .alias('INFANT_NICU_ADMISSION_RECODE')]\n",
    "    ).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the 'INFANT_NICU_ADMISSION' column and we pass the name of the\n",
    "feature to the rec_integer method. We also alias the newly transformed column as\n",
    "'INFANT_NICU_ADMISSION_RECODE'. This way we will also confirm that our UDF\n",
    "works as intended.  \n",
    "\n",
    "So, to transform all the YNU_cols in one go, we will create a list of such\n",
    "transformations, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs_YNU = [\n",
    "    rec_integer(x, func.lit('YNU')).alias(x)\n",
    "    if x in YNU_cols\n",
    "    else x\n",
    "    for x in births_transformed.columns\n",
    "]\n",
    "\n",
    "births_transformed = births_transformed.select(exprs_YNU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we got it correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+------------+-------------+------------------+\n",
      "|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
      "+------------+-------------+------------+-------------+------------------+\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "|           0|            0|           0|            0|                 1|\n",
      "|           0|            0|           0|            0|                 0|\n",
      "+------------+-------------+------------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "births_transformed.select(YNU_cols[-5:]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like everything worked as we wanted it to work, so let's get to know our\n",
    "data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to know your data  \n",
    "\n",
    "In order to build a statistical model in an informed way, an intimate knowledge of\n",
    "the dataset is necessary. Without knowing the data it is possible to build a successful\n",
    "model, but it is then a much more arduous task, or it would require more technical\n",
    "resources to test all the possible combinations of features. Therefore, after spending\n",
    "the required 80% of the time cleaning the data, we spend the next 15% getting to\n",
    "know it!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics  \n",
    "\n",
    "I normally start with descriptive statistics. Even though the DataFrames expose\n",
    "the .describe() method, since we are working with MLlib, we will use the\n",
    ".colStats(...) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[A word of warning: the .colStats(...) calculates the descriptive\n",
    "statistics based on a sample. For real world datasets this should not really\n",
    "matter but if your dataset has less than 100 observations you might get\n",
    "some strange results.]*  \n",
    "\n",
    "The method takes an RDD of data to calculate the descriptive statistics of and return\n",
    "a MultivariateStatisticalSummary object that contains the following descriptive\n",
    "statistics:  \n",
    "\n",
    "• count(): This holds a row count  \n",
    "• max(): This holds maximum value in the column  \n",
    "• mean(): This holds the value of the mean for the values in the column  \n",
    "• min(): This holds the minimum value in the column  \n",
    "• normL1(): This holds the value of the L1-Norm for the values in the column  \n",
    "• normL2(): This holds the value of the L2-Norm for the values in the column  \n",
    "• numNonzeros(): This holds the number of nonzero values in the column  \n",
    "• variance(): This holds the value of the variance for the values in the column    \n",
    "\n",
    "The\n",
    "following is a snippet that calculates the descriptive statistics of the numeric features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOTHER_AGE_YEARS: \t28.30 \t 6.08\n",
      "FATHER_COMBINED_AGE: \t44.55 \t 27.55\n",
      "CIG_BEFORE: \t1.43 \t 5.18\n",
      "CIG_1_TRI: \t0.91 \t 3.83\n",
      "CIG_2_TRI: \t0.70 \t 3.31\n",
      "CIG_3_TRI: \t0.58 \t 3.11\n",
      "MOTHER_HEIGHT_IN: \t65.12 \t 6.45\n",
      "MOTHER_PRE_WEIGHT: \t214.50 \t 210.21\n",
      "MOTHER_DELIVERY_WEIGHT: \t223.63 \t 180.01\n",
      "MOTHER_WEIGHT_GAIN: \t30.74 \t 26.23\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.stat as st\n",
    "import numpy as np \n",
    "\n",
    "numeric_cols = ['MOTHER_AGE_YEARS','FATHER_COMBINED_AGE',\n",
    "                'CIG_BEFORE','CIG_1_TRI','CIG_2_TRI','CIG_3_TRI',\n",
    "                'MOTHER_HEIGHT_IN','MOTHER_PRE_WEIGHT',\n",
    "                'MOTHER_DELIVERY_WEIGHT','MOTHER_WEIGHT_GAIN'\n",
    "            ]\n",
    "\n",
    "numeric_rdd = births_transformed.select(numeric_cols).rdd.map(lambda row: [e for e in row])\n",
    "mllib_stats = st.Statistics.colStats(numeric_rdd)\n",
    "\n",
    "for col, m, v in zip(numeric_cols, mllib_stats.mean(), mllib_stats.variance()):\n",
    "    print('{0}: \\t{1:.2f} \\t {2:.2f}'.format(col, m, np.sqrt(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, mothers, compared to fathers, are younger: the average age of\n",
    "mothers was 28 versus over 44 for fathers. A good indication (at least for some of the\n",
    "infants) was that many mothers quit smoking while being pregnant; it is horrifying,\n",
    "though, that there still were some that continued smoking.  \n",
    "\n",
    "For the categorical variables, we will calculate the frequencies of their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFANT_ALIVE_AT_REPORT [(1, 23349), (0, 22080)]\n",
      "BIRTH_PLACE [('1', 44558), ('4', 327), ('3', 224), ('2', 136), ('7', 91), ('5', 74), ('6', 11), ('9', 8)]\n",
      "DIABETES_PRE [(0, 44881), (1, 548)]\n",
      "DIABETES_GEST [(0, 43451), (1, 1978)]\n",
      "HYP_TENS_PRE [(0, 44348), (1, 1081)]\n",
      "HYP_TENS_GEST [(0, 43302), (1, 2127)]\n",
      "PREV_BIRTH_PRETERM [(0, 43088), (1, 2341)]\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [e for e in births_transformed.columns if e not in numeric_cols]\n",
    "\n",
    "categorical_rdd = births_transformed.select(categorical_cols).rdd.map(lambda row: [e for e in row])\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    agg = categorical_rdd.groupBy(lambda row: row[i]).map(lambda row: (row[0], len(row[1])))\n",
    "    \n",
    "    print(col, sorted(agg.collect(), key=lambda el: el[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the deliveries happened in hospital (BIRTH_PLACE equal to 1). Around 550\n",
    "deliveries happened at home: some intentionally ('BIRTH_PLACE' equal to 3), and\n",
    "some not ('BIRTH_PLACE' equal to 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations  \n",
    "\n",
    "Correlations help to identify collinear numeric features and handle them\n",
    "appropriately. Let's check the correlations between our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIG_BEFORE-to-CIG_1_TRI: 0.83\n",
      "CIG_BEFORE-to-CIG_2_TRI: 0.72\n",
      "CIG_BEFORE-to-CIG_3_TRI: 0.62\n",
      "CIG_1_TRI-to-CIG_BEFORE: 0.83\n",
      "CIG_1_TRI-to-CIG_2_TRI: 0.87\n",
      "CIG_1_TRI-to-CIG_3_TRI: 0.76\n",
      "CIG_2_TRI-to-CIG_BEFORE: 0.72\n",
      "CIG_2_TRI-to-CIG_1_TRI: 0.87\n",
      "CIG_2_TRI-to-CIG_3_TRI: 0.89\n",
      "CIG_3_TRI-to-CIG_BEFORE: 0.62\n",
      "CIG_3_TRI-to-CIG_1_TRI: 0.76\n",
      "CIG_3_TRI-to-CIG_2_TRI: 0.89\n",
      "MOTHER_PRE_WEIGHT-to-MOTHER_DELIVERY_WEIGHT: 0.54\n",
      "MOTHER_PRE_WEIGHT-to-MOTHER_WEIGHT_GAIN: 0.65\n",
      "MOTHER_DELIVERY_WEIGHT-to-MOTHER_PRE_WEIGHT: 0.54\n",
      "MOTHER_DELIVERY_WEIGHT-to-MOTHER_WEIGHT_GAIN: 0.60\n",
      "MOTHER_WEIGHT_GAIN-to-MOTHER_PRE_WEIGHT: 0.65\n",
      "MOTHER_WEIGHT_GAIN-to-MOTHER_DELIVERY_WEIGHT: 0.60\n"
     ]
    }
   ],
   "source": [
    "corrs = st.Statistics.corr(numeric_rdd)\n",
    "\n",
    "for i, el in enumerate(corrs > 0.5):\n",
    "    correlated = [\n",
    "        (numeric_cols[j], corrs[i][j]) for j, e in enumerate(el) if e == 1.0 and j != i\n",
    "    ]\n",
    "    \n",
    "    if len(correlated) > 0:\n",
    "        for e in correlated:\n",
    "            print('{0}-to-{1}: {2:.2f}'.format(numeric_cols[i], e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the 'CIG_...' features are highly correlated, so we can drop most of\n",
    "them. Since we want to predict the survival chances of an infant as soon as possible,\n",
    "we will keep only the 'CIG_1_TRI'. Also, as expected, the weight features are also\n",
    "highly correlated and we will only keep the 'MOTHER_PRE_WEIGHT':  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = [\n",
    "    'INFANT_ALIVE_AT_REPORT', \n",
    "    'BIRTH_PLACE', \n",
    "    'MOTHER_AGE_YEARS', \n",
    "    'FATHER_COMBINED_AGE', \n",
    "    'CIG_1_TRI', \n",
    "    'MOTHER_HEIGHT_IN', \n",
    "    'MOTHER_PRE_WEIGHT', \n",
    "    'DIABETES_PRE', \n",
    "    'DIABETES_GEST', \n",
    "    'HYP_TENS_PRE', \n",
    "    'HYP_TENS_GEST', \n",
    "    'PREV_BIRTH_PRETERM'\n",
    "]\n",
    "\n",
    "births_transformed = births_transformed.select([e for e in features_to_keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical testing  \n",
    "\n",
    "We cannot calculate correlations for the categorical features. However, we can run a\n",
    "Chi-square test to determine if there are significant differences.  \n",
    "\n",
    "Here's how you can do it using the .chiSqTest(...) method of MLlib:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIRTH_PLACE 0.0\n",
      "DIABETES_PRE 0.0\n",
      "DIABETES_GEST 0.0\n",
      "HYP_TENS_PRE 0.0\n",
      "HYP_TENS_GEST 0.0\n",
      "PREV_BIRTH_PRETERM 0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.linalg as ln\n",
    "\n",
    "for cat in categorical_cols[1:]:\n",
    "    agg = births_transformed.groupBy('INFANT_ALIVE_AT_REPORT').pivot(cat).count()\n",
    "    \n",
    "    agg_rdd = agg.rdd.map(lambda row: (row[1:])).flatMap(lambda row: [0 if e == None else e for e in row]).collect()\n",
    "    \n",
    "    row_length = len(agg.collect()[0]) -1\n",
    "    agg = ln.Matrices.dense(row_length, 2, agg_rdd)\n",
    "    \n",
    "    test = st.Statistics.chiSqTest(agg)\n",
    "    print(cat, round(test.pValue, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop through all the categorical variables and pivot them by the 'INFANT_ALIVE_\n",
    "AT_REPORT' feature to get the counts. Next, we transform them into an RDD, so we\n",
    "can then convert them into a matrix using the pyspark.mllib.linalg module. The\n",
    "first parameter to the .Matrices.dense(...) method specifies the number of rows\n",
    "in the matrix; in our case, it is the length of distinct values of the categorical feature.  \n",
    "\n",
    "The second parameter specifies the number of columns: we have two as our\n",
    "'INFANT_ALIVE_AT_REPORT' target variable has only two values.  \n",
    "\n",
    "The last parameter is a list of values to be transformed into a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our counts in a matrix form, we can use the .chiSqTest(...) to\n",
    "calculate our test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Finally Our tests reveal that all the features should be significantly different and should help\n",
    "us predict the chance of survival of an infant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the final dataset  \n",
    "\n",
    "Therefore, it is time to create our final dataset that we will use to build our models.\n",
    "We will convert our DataFrame into an RDD of LabeledPoints.  \n",
    "\n",
    "A LabeledPoint is a MLlib structure that is used to train the machine learning\n",
    "models. It consists of two attributes: label and features.  \n",
    "\n",
    "The label is our target variable and features can be a NumPy array, list,\n",
    "pyspark.mllib.linalg.SparseVector, pyspark.mllib.linalg.DenseVector, or\n",
    "scipy.sparse column matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Creating an RDD of LabeledPoints**  \n",
    "\n",
    "Before we build our final dataset, we first need to deal with one final obstacle: our\n",
    "'BIRTH_PLACE' feature is still a string. While any of the other categorical variables\n",
    "can be used as is (as they are now dummy variables), we will use a hashing trick to\n",
    "encode the 'BIRTH_PLACE' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.mllib.feature as ft \n",
    "import pyspark.mllib.regression as reg \n",
    "\n",
    "hashing = ft.HashingTF(7)\n",
    "\n",
    "births_hashed = births_transformed.rdd.map(lambda row: [\n",
    "    list(hashing.transform(row[1]).toArray())\n",
    "    if col == 'BIRTH_PLACE' else row[i]\n",
    "    for i, col in enumerate(features_to_keep)]).map(lambda row: [[e] if type(e) == int else e for e in row]).map(\n",
    "        lambda row: [item for sublist in row for item in sublist]).map(\n",
    "            lambda row: reg.LabeledPoint(row[0], ln.Vectors.dense(row[1:]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create the hashing model. Our feature has seven levels, so we use as many\n",
    "features as that for the hashing trick. Next, we actually use the model to convert\n",
    "our 'BIRTH_PLACE' feature into a SparseVector; such a data structure is preferred\n",
    "if your dataset has many columns but in a row only a few of them have non-zero\n",
    "values. We then combine all the features together and finally create a LabeledPoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training and testing  \n",
    "Before we move to the modeling stage, we need to split our dataset into two sets: one\n",
    "we'll use for training and the other for testing. Luckily, RDDs have a handy method\n",
    "to do just that: .randomSplit(...). The method takes a list of proportions that are\n",
    "to be used to randomly split the dataset.  \n",
    "\n",
    "Here is how it is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_train, births_test = births_hashed.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting infant survival  \n",
    "\n",
    "Finally, we can move to predicting the infants' survival chances. In this section, we\n",
    "will build two models: a linear classifier—the logistic regression, and a non-linear\n",
    "one—a random forest. For the former one, we will use all the features at our disposal,\n",
    "whereas for the latter one, we will employ a ChiSqSelector(...) method to select\n",
    "the top four features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression in MLlib  \n",
    "\n",
    "Logistic regression is somewhat a benchmark to build any classification model.\n",
    "MLlib used to provide a logistic regression model estimated using a stochastic\n",
    "gradient descent (SGD) algorithm. This model has been deprecated in Spark 2.0 in\n",
    "favor of the LogisticRegressionWithLBFGS model.  \n",
    "\n",
    "The LogisticRegressionWithLBFGS model uses the Limited-memory Broyden–\n",
    "Fletcher–Goldfarb–Shanno (BFGS) optimization algorithm. It is a quasi-Newton\n",
    "method that approximates the BFGS algorithm.  \n",
    "\n",
    "*[For those of you who are mathematically adept and interested in this,\n",
    "we suggest perusing this blog post that is a nice walk-through of the\n",
    "optimization algorithms: http://aria42.com/blog/2014/12/understanding-lbfgs.]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "LR_Model = LogisticRegressionWithLBFGS.train(births_train, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model is very simple: we just need to call the .train(...) method.\n",
    "The required parameters are the RDD with LabeledPoints; we also specified the\n",
    "number of iterations so it does not take too long to run.  \n",
    "\n",
    "Having trained the model using the births_train dataset, let's use the model to\n",
    "predict the classes for our testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_results = (\n",
    "        births_test.map(lambda row: row.label) \\\n",
    "        .zip(LR_Model \\\n",
    "             .predict(births_test\\\n",
    "                      .map(lambda row: row.features)))\n",
    "    ).map(lambda row: (row[0], row[1] * 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding snippet creates an RDD where each element is a tuple, with the first\n",
    "element being the actual label and the second one, the model's prediction.  \n",
    "\n",
    "MLlib provides an evaluation metric for classification and regression. Let's check\n",
    "how well or how bad our model performed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.mllib.evaluation as ev\n",
    "# LR_evaluation = ev.BinaryClassificationMetrics(LR_results)\n",
    "\n",
    "# print('Area under PR: {0:.2f}' \\\n",
    "#       .format(LR_evaluation.areaUnderPR))\n",
    "# print('Area under ROC: {0:.2f}' \\\n",
    "#       .format(LR_evaluation.areaUnderROC))\n",
    "# LR_evaluation.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performed reasonably well! The 85% area under the Precision-Recall\n",
    "curve indicates a good fit. In this case, we might be getting slightly more predicted\n",
    "deaths (true and false positives). In this case, this is actually a good thing as it would\n",
    "allow doctors to put the expectant mother and the infant under special care.  \n",
    "\n",
    "\n",
    "The area under Receiver-Operating Characteristic (ROC) can be understood as a\n",
    "probability of the model ranking higher than a randomly chosen positive instance\n",
    "compared to a randomly chosen negative one. A 63% value can be thought of\n",
    "as acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting only the most predictable features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any model that uses less features to predict a class accurately should always be\n",
    "preferred to a more complex one. MLlib allows us to select the most predictable\n",
    "features using a Chi-Square selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ft.ChiSqSelector(4).fit(births_train)\n",
    "\n",
    "topFeatures_train = (\n",
    "        births_train.map(lambda row: row.label) \\\n",
    "        .zip(selector \\\n",
    "             .transform(births_train \\\n",
    "                        .map(lambda row: row.features)))\n",
    "    ).map(lambda row: reg.LabeledPoint(row[0], row[1]))\n",
    "\n",
    "topFeatures_test = (\n",
    "        births_test.map(lambda row: row.label) \\\n",
    "        .zip(selector \\\n",
    "             .transform(births_test \\\n",
    "                        .map(lambda row: row.features)))\n",
    "    ).map(lambda row: reg.LabeledPoint(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We asked the selector to return the four most predictive features from the dataset\n",
    "and train the selector using the births_train dataset. We then used the model to\n",
    "extract only those features from our training and testing datasets.\n",
    "\n",
    "\n",
    "The .ChiSqSelector(...) method can only be used for numerical features;\n",
    "categorical variables need to be either hashed or dummy coded before the selector\n",
    "can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest in MLlib  \n",
    "We are now ready to build the random forest model.\n",
    "The following code shows you how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "RF_model = RandomForest \\\n",
    "    .trainClassifier(data=topFeatures_train, \n",
    "                     numClasses=2, \n",
    "                     categoricalFeaturesInfo={}, \n",
    "                     numTrees=6,  \n",
    "                     featureSubsetStrategy='all',\n",
    "                     seed=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter to the .trainClassifier(...) method specifies the training\n",
    "dataset. The numClasses one indicates how many classes our target variable has.\n",
    "As the third parameter, you can pass a dictionary where the key is the index of a\n",
    "categorical feature in our RDD and the value for the key indicates the number of\n",
    "levels that the categorical feature has. The numTrees specifies the number of trees\n",
    "to be in the forest. The next parameter tells the model to use all the features in our\n",
    "dataset instead of keeping only the most descriptive ones, while the last one specifies\n",
    "the seed for the stochastic part of the model.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_Model_2 = LogisticRegressionWithLBFGS \\\n",
    "    .train(topFeatures_train, iterations=10)\n",
    "\n",
    "LR_results_2 = (\n",
    "        topFeatures_test.map(lambda row: row.label) \\\n",
    "        .zip(LR_Model_2 \\\n",
    "             .predict(topFeatures_test \\\n",
    "                      .map(lambda row: row.features)))\n",
    "    ).map(lambda row: (row[0], row[1] * 1.0))\n",
    "\n",
    "LR_evaluation_2 = ev.BinaryClassificationMetrics(LR_results_2)\n",
    "\n",
    "print('Area under PR: {0:.2f}' \\\n",
    "      .format(LR_evaluation_2.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    "      .format(LR_evaluation_2.areaUnderROC))\n",
    "LR_evaluation_2.unpersist()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Random Forest model with fewer features performed even\n",
    "better than the logistic regression model. Let's see how the logistic regression would\n",
    "perform with a reduced number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_Model_2 = LogisticRegressionWithLBFGS \\\n",
    ".train(topFeatures_train, iterations=10)\n",
    "LR_results_2 = (\n",
    "topFeatures_test.map(lambda row: row.label) \\\n",
    ".zip(LR_Model_2 \\\n",
    ".predict(topFeatures_test \\\n",
    ".map(lambda row: row.features)))\n",
    ").map(lambda row: (row[0], row[1] * 1.0))\n",
    "LR_evaluation_2 = ev.BinaryClassificationMetrics(LR_results_2)\n",
    "print('Area under PR: {0:.2f}' \\\n",
    ".format(LR_evaluation_2.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    ".format(LR_evaluation_2.areaUnderROC))\n",
    "LR_evaluation_2.unpersist()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, both models can be simplified and still attain the same level of\n",
    "accuracy. Having said that, you should always opt for a model with fewer variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "In this chapter, we looked at the capabilities of the MLlib package of PySpark.\n",
    "Even though the package is currently in a maintenance mode and is not actively\n",
    "being worked on, it is still good to know how to use it. Also, for now it is the only\n",
    "package available to train models while streaming data. We used MLlib to clean up,\n",
    "transform, and get familiar with the dataset of infant deaths. Using that knowledge\n",
    "we then successfully built two models that aimed at predicting the chance of infant\n",
    "survival given the information about its mother, father, and place of birth.\n",
    "In the next chapter, we will revisit the same problem, but using the newer package\n",
    "that is currently the Spark recommended package for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_envi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
