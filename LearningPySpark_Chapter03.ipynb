{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(\"C:/Program Files/spark-3.5.4-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jre1.8.0_431\" \n",
    "os.environ[\"SPARK_HOME\"] = \"C:/Program Files/spark-3.5.4-bin-hadoop3\" \n",
    "os.environ['HADOOP_HOME '] = 'C:/Program Files/hadoop-3.4.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Session\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accéder au SparkContext à partir de SparkSession\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames  \n",
    "\n",
    "A DataFrame is an immutable distributed collection of data that is organized into named columns analogous to a table in a relational database. Introduced as an experimental feature within Apache Spark 1.0 as SchemaRDD, they were renamed to DataFrames as part of the Apache Spark 1.3 release. For readers who are familiar with Python Pandas DataFrame or R DataFrame, a Spark DataFrame is a similar concept in that it allows users to easily work with structured data (for example, data tables); there are some differences as well so please temper your expectations.  \n",
    "\n",
    "By imposing a structure onto a distributed collection of data, this allows Spark users to query structured data in Spark SQL or using expression methods (instead of lambdas). By structuring your data, this allows the Apache Spark engine – specifically, the Catalyst Optimizer – to significantly improve the performance of Spark queries. In earlier APIs of Spark (that is, RDDs), executing queries in Python could be significantly slower due to communication overhead between the Java JVM and Py4J.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python to RDD communications  \n",
    "\n",
    "Whenever a PySpark program is executed using RDDs, there is a potentially large\n",
    "overhead to execute the job. As noted in the following diagram, in the PySpark\n",
    "driver, the Spark Context uses Py4j to launch a JVM using the JavaSparkContext.\n",
    "Any RDD transformations are initially mapped to PythonRDD objects in Java.  \n",
    "\n",
    "Once these tasks are pushed out to the Spark Worker(s), PythonRDD objects launch\n",
    "Python subprocesses using pipes to send both code and data to be processed within\n",
    "Python:  \n",
    "\n",
    "While this approach allows PySpark to distribute the processing of the data to\n",
    "multiple Python subprocesses on multiple workers, as you can see, there is a lot of\n",
    "context switching and communications overhead between Python and the JVM.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catalyst Optimizer refresh  \n",
    "\n",
    "As noted in Chapter 1, Understanding Spark, one of the primary reasons the Spark\n",
    "SQL engine is so fast is because of the Catalyst Optimizer. For readers with a\n",
    "database background, this diagram looks similar to the logical/physical planner\n",
    "and cost model/cost-based optimization of a relational database management\n",
    "system (RDBMS).\n",
    "\n",
    "The significance of this is that, as opposed to immediately processing the query, the\n",
    "Spark engine's Catalyst Optimizer compiles and optimizes a logical plan and has a\n",
    "cost optimizer that determines the most efficient physical plan generated.  \n",
    "\n",
    "As previously noted, the optimizer is based on functional programming\n",
    "constructs and was designed with two purposes in mind: to ease the adding of new\n",
    "optimization techniques and features to Spark SQL, and to allow external developers\n",
    "to extend the optimizer (for example, adding data-source-specific rules, support for\n",
    "new data types, and so on).  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up PySpark with DataFrames  \n",
    "\n",
    "The significance of DataFrames and the Catalyst Optimizer (and Project Tungsten) is\n",
    "the increase in performance of PySpark queries when compared to non-optimized\n",
    "RDD queries. Prior to the introduction of\n",
    "DataFrames, Python query speeds were often twice as slow as the same Scala\n",
    "queries using RDD. Typically, this slowdown in query performance was due\n",
    "to the communications overhead between Python and the JVM.  \n",
    "\n",
    "With DataFrames, not only was there a significant improvement in Python\n",
    "performance, there is now performance parity between Python, Scala, SQL, and R.  \n",
    "\n",
    "*[It is important to note that while, with DataFrames, PySpark is often\n",
    "significantly faster, there are some exceptions. The most prominent one\n",
    "is the use of Python UDFs, which results in round-trip communication\n",
    "between Python and the JVM. Note, this would be the worst-case scenario\n",
    "which would be similar if the compute was done on RDDs.]*  \n",
    "\n",
    "Python can take advantage of the performance optimizations in Spark even while\n",
    "the codebase for the Catalyst Optimizer is written in Scala. Basically, it is a Python\n",
    "wrapper of approximately 2,000 lines of code that allows PySpark DataFrame queries\n",
    "to be significantly faster.  \n",
    "\n",
    "Altogether, Python DataFrames (as well as SQL, Scala DataFrames, and R\n",
    "DataFrames) are all able to make use of the Catalyst Optimizer  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames  \n",
    "\n",
    "Typically, you will create DataFrames by importing data using SparkSession\n",
    "(or calling spark in the PySpark shell).  \n",
    "\n",
    "First, instead of accessing the file system, we will create a DataFrame by generating\n",
    "the data. In this case, we'll first create the stringJSONRDD RDD and then convert it\n",
    "into a DataFrame. This code snippet creates an RDD comprised of swimmers (their\n",
    "ID, name, age, and eye color) in JSON format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating our own JSON data  \n",
    "\n",
    "Below, we will generate initially generate the stringJSONRDD RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our own DataFrame\n",
    "# This way we don't have to access the file system yet.\n",
    "stringJSONRDD = sc.parallelize((\"\"\" \n",
    "  { \"id\": \"123\",\n",
    "    \"name\": \"Katie\",\n",
    "    \"age\": 19,\n",
    "    \"eyeColor\": \"brown\"\n",
    "  }\"\"\",\n",
    "   \"\"\"{\n",
    "    \"id\": \"234\",\n",
    "    \"name\": \"Michael\",\n",
    "    \"age\": 22,\n",
    "    \"eyeColor\": \"green\"\n",
    "  }\"\"\", \n",
    "  \"\"\"{\n",
    "    \"id\": \"345\",\n",
    "    \"name\": \"Simone\",\n",
    "    \"age\": 23,\n",
    "    \"eyeColor\": \"blue\"\n",
    "  }\"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the RDD, we will convert this into a DataFrame by using\n",
    "the SparkSession read.json method (that is, spark.read.json(...)). We will also\n",
    "create a temporary table by using the .createOrReplaceTempView method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is the code to create a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmersJSON = spark.read.json(stringJSONRDD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is the code for creating a temporary table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the previous chapters, many RDD operations are transformations, which\n",
    "are not executed until an action operation is executed. For example, in the preceding\n",
    "code snippet, the sc.parallelize is a transformation that is executed when\n",
    "converting from an RDD to a DataFrame by using spark.read.json.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that parallelize, map, and mapPartitions are all RDD\n",
    "transformations. Wrapped within the DataFrame operation, spark.read.json (in\n",
    "this case), are not only the RDD transformations, but also the action which converts\n",
    "the RDD into a DataFrame. This is an important call out, because even though you\n",
    "are executing DataFrame operations, to debug your operations you will need to\n",
    "remember that you will be making sense of RDD operations within the Spark UI.  \n",
    "\n",
    "Note that creating the temporary table is a DataFrame transformation and not\n",
    "executed until a DataFrame action is executed.  \n",
    "\n",
    "*[DataFrame transformations and actions are similar to RDD\n",
    "transformations and actions in that there is a set of operations that\n",
    "are lazy (transformations). But, in comparison to RDDs, DataFrames\n",
    "operations are not as lazy, primarily due to the Catalyst Optimizer.]*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple DataFrame queries  \n",
    "\n",
    "Now that you have created the swimmersJSON DataFrame, we will be able to run\n",
    "the DataFrame API, as well as SQL queries against it. Let's start with a simple query\n",
    "showing all the rows within the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DataFrame API query  \n",
    "To do this using the DataFrame API, you can use the show(<n>) method, which\n",
    "prints the first n rows to the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "|age|eyeColor| id|   name|\n",
      "+---+--------+---+-------+\n",
      "| 19|   brown|123|  Katie|\n",
      "| 22|   green|234|Michael|\n",
      "| 23|    blue|345| Simone|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame API  \n",
    "swimmersJSON.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **SQL query**  \n",
    "If you prefer writing SQL statements, you can write the following query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
       " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
       " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM swimmersJSON\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the .collect() method, which returns all the records as a list of\n",
    "Row objects. Note that you can use either the collect() or show() method for\n",
    "both DataFrames and SQL queries. Just make sure that if you use .collect(),\n",
    "this is for a small DataFrame, since it will return all of the rows in the DataFrame\n",
    "and move them back from the executors to the driver. You can instead use\n",
    "take(<n>) or show(<n>), which allow you to limit the number of rows returned\n",
    "by specifying <n>.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interoperating with RDDs\n",
    "\n",
    "There are two different methods for converting existing RDDs to DataFrames (or\n",
    "Datasets[T]): inferring the schema using reflection, or programmatically specifying\n",
    "the schema. The former allows you to write more concise code (when your Spark\n",
    "application already knows the schema), while the latter allows you to construct\n",
    "DataFrames when the columns and their data types are only revealed at run time.\n",
    "Note, reflection is in reference to schema reflection as opposed to Python reflection.\n",
    "\n",
    "* **Inferring the schema using reflection**  \n",
    "In the process of building the DataFrame and running the queries, we skipped over\n",
    "the fact that the schema for this DataFrame was automatically defined. Initially, row\n",
    "objects are constructed by passing a list of key/value pairs as **kwargs to the row\n",
    "class. Then, Spark SQL converts this RDD of row objects into a DataFrame, where\n",
    "the keys are the columns and the data types are inferred by sampling the data.  \n",
    "\n",
    "\n",
    "`[The '**kwargs' construct allows you to pass a variable number of\n",
    "parameters to a method at runtime.]` \n",
    "\n",
    "Going back to the code, after initially creating the swimmersJSON DataFrame,\n",
    "without specifying the schema, you will notice the schema definition by using\n",
    "the printSchema() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema\n",
    "swimmersJSON.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want to specify the schema because, in this example, we know that\n",
    "the id is actually a long instead of a string?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Programmatically specifying the schema**  \n",
    "In this case, let's programmatically specify the schema by bringing in Spark SQL data\n",
    "types (pyspark.sql.types) and generate some .csv data for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import types \n",
    "from pyspark.sql.types import * \n",
    "\n",
    "# Generate comma delimited data \n",
    "stringCSVRDD = sc.parallelize([\n",
    "(123, 'Katie', 19, 'brown'),\n",
    "(234, 'Michael', 22, 'green'),\n",
    "(345, 'Simone', 23, 'blue')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will encode the schema as a string, per the [schema] variable below. Then\n",
    "we will define the schema using StructType and StructField."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType, LongType\n",
    "\n",
    "schemaString = \"id name age eyeColor\"\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),    \n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the StructField class is broken down in terms of:  \n",
    "• name: The name of this field  \n",
    "• dataType: The data type of this field  \n",
    "• nullable: Indicates whether values of this field can be null  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will apply the schema (schema) we created to the stringCSVRDD RDD\n",
    "(that is, the generated.csv data) and create a temporary view so we can query it\n",
    "using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the schema to the RDD and create DataFrame \n",
    "swimmers = spark.createDataFrame(stringCSVRDD, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view using the dataframe\n",
    "swimmers.createOrReplaceTempView(\"swimmers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this example, we have finer-grain control over the schema and can specify that\n",
    "id is a long (as opposed to a string in the previous section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- eyeColor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swimmers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying with the DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the previous section, you can start off by using collect(), show(), or\n",
    "take() to view the data within your DataFrame (with the last two including the\n",
    "option to limit the number of returned rows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Number of rows**    \n",
    "To get the number of rows within your DataFrame, you can use the count() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmers.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Running filter statements**  \n",
    "To run a filter statement, you can use the filter clause; in the following code\n",
    "snippet, we are using the select clause to specify the columns to be returned\n",
    "as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the id, age where age = 22\n",
    "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way to write the above query is below\n",
    "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only want to get back the name of the swimmers who have an eye color\n",
    "that begins with the letter b, we can use a SQL-like syntax, like, as shown in\n",
    "the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|eyeColor|\n",
      "+------+--------+\n",
      "| Katie|   brown|\n",
      "|Simone|    blue|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the name, eyeColor where eyeColor like '%b'\n",
    "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying with SQL  \n",
    "\n",
    "Let's run the same queries, except this time, we will do so using SQL queries against\n",
    "the same DataFrame. Recall that this DataFrame is accessible because we executed\n",
    "the .createOrReplaceTempView method for swimmers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Number of rows**  \n",
    "The following is the code snippet to get the number of rows within your DataFrame\n",
    "using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(1) FROM swimmers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Running filter statements using the where Clauses**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a filter statement using SQL, you can use the where clause, as noted in the\n",
    "following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|234| 22|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, age FROM swimmers WHERE age = 22\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the DataFrame API querying, if we want to get back the name of the\n",
    "swimmers who have an eye color that begins with the letter b only, we can use\n",
    "the like syntax as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|name|eyeColor|\n",
      "+----+--------+\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT name, eyeColor FROM swimmers WHERE eyeColor LIKE '%b'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`*An important note when working with Spark SQL and DataFrames is that\n",
    "while it is easy to work with CSV, JSON, and a variety of data formats,\n",
    "the most common storage format for Spark SQL analytics queries is the\n",
    "Parquet file format. It is a columnar format that is supported by many\n",
    "other data processing systems and Spark SQL supports both reading\n",
    "and writing Parquet files that automatically preserves the schema of\n",
    "the original data.*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame scenario – on-time flight performance  \n",
    "\n",
    "To showcase the types of queries you can do with DataFrames, let's look at the use\n",
    "case of on-time flight performance. We will analyze the Airline On-Time Performance\n",
    "and Causes of Flight Delays: On-Time Data (http://bit.ly/2ccJPPM), and join this\n",
    "with the airports dataset, obtained from the Open Flights Airport, airline, and route\n",
    "data (http://bit.ly/2ccK5hw), to better understand the variables associated with\n",
    "flight delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Preparing the source datasets**  \n",
    "We will first process the source airports and flight performance datasets by\n",
    "specifying their file path location and importing them using SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set File Paths\n",
    "flightPerfFilePath =\"datasets/departuredelays.csv\"\n",
    "airportsFilePath =\"datasets/airport-codes-na.txt\"\n",
    "\n",
    "# Obtain Airports dataset\n",
    "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "# Obtain Departure Delays dataset\n",
    "flightPerf = spark.read.csv(flightPerfFilePath, header='true')\n",
    "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
    "\n",
    "# Cache the Departure Delays dataset\n",
    "flightPerf.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that we're importing the data using the CSV reader (com.databricks.spark.\n",
    "csv), which works for any specified delimiter (note that the airports data is tabdelimited,\n",
    "while the flight performance data is comma-delimited). Finally, we cache\n",
    "the flight dataset so subsequent queries will be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Joining flight performance and airports**  \n",
    "\n",
    "One of the more common tasks with DataFrames/SQL is to join two different\n",
    "datasets; it is often one of the more demanding operations (from a performance\n",
    "perspective). With DataFrames, a lot of the performance optimizations for these\n",
    "joins are included by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|  Delays|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
    "spark.sql(\"\"\"\n",
    "          SELECT a.City, f.origin, SUM(f.delay) AS Delays\n",
    "          FROM FlightPerformance f\n",
    "          JOIN airports a\n",
    "          ON a.IATA = f.origin\n",
    "          WHERE a.State = 'WA'\n",
    "          GROUP BY a.City, f.origin\n",
    "          ORDER BY SUM(f.delay) DESC\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our scenario, we are querying the total delays by city and origin code for the state\n",
    "of Washington. This will require joining the flight performance data with the airports\n",
    "data by International Air Transport Association (IATA) code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Visualizing our flight-performance data**  \n",
    "Let's continue visualizing our data, but broken down by all states in the continental US:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|State|   Delays|\n",
      "+-----+---------+\n",
      "|   SC|  80666.0|\n",
      "|   LA| 199136.0|\n",
      "|   MN| 256811.0|\n",
      "|   NJ| 452791.0|\n",
      "|   OR| 109333.0|\n",
      "|   VA|  98016.0|\n",
      "| NULL| 397237.0|\n",
      "|   WY|  15365.0|\n",
      "|   MI| 366486.0|\n",
      "|   NV| 474208.0|\n",
      "|   WI| 152311.0|\n",
      "|   ID|  22932.0|\n",
      "|   CA|1891919.0|\n",
      "|   CT|  54662.0|\n",
      "|   MT|  19271.0|\n",
      "|   NC| 394256.0|\n",
      "|   VT|  14755.0|\n",
      "|   MD| 362845.0|\n",
      "|   IL|1630792.0|\n",
      "|   ME|  15214.0|\n",
      "+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Query Sum of Flight Delays by State (for the US)\n",
    "spark.sql(\"\"\"\n",
    "          SELECT a.State, SUM(f.delay) as Delays\n",
    "          FROM FlightPerformance f\n",
    "          JOIN airports a\n",
    "          ON a.IATA = f.origin\n",
    "          WHERE a.Country = 'USA'\n",
    "          GROUP BY a.state\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key benefits of DataFrames is that the information is structured similar to\n",
    "a table. Therefore, whether you are using notebooks or your favorite BI tool, you will\n",
    "be able to quickly visualize your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Dataset API  \n",
    "After this discussion about Spark DataFrames, let's have a quick recap of the Spark\n",
    "Dataset API. Introduced in Apache Spark 1.6, the goal of Spark Datasets was to\n",
    "provide an API that allows users to easily express transformations on domain\n",
    "objects, while also providing the performance and benefits of the robust Spark SQL\n",
    "execution engine. As part of the Spark 2.0 release (and as noted in the diagram\n",
    "below), the DataFrame APIs is merged into the Dataset API thus unifying data\n",
    "processing capabilities across all libraries. Because of this unification, developers\n",
    "now have fewer concepts to learn or remember, and work with a single high-level\n",
    "and type-safe API – called Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, the Spark DataFrame is an alias for a collection of generic objects\n",
    "Dataset[Row], where a Row is a generic untyped JVM object. Dataset, by contrast, is\n",
    "a collection of strongly-typed JVM objects, dictated by a case class you define, in Scala\n",
    "or Java. This last point is particularly important as this means that the Dataset API\n",
    "is not supported by PySpark due to the lack of benefit from the type enhancements.\n",
    "Note, for the parts of the Dataset API that are not available in PySpark, they can be\n",
    "accessed by converting to an RDD or by using UDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "With Spark DataFrames, Python developers can make use of a simpler abstraction\n",
    "layer that is also potentially significantly faster. One of the main reasons Python is\n",
    "initially slower within Spark is due to the communication layer between Python\n",
    "sub-processes and the JVM. For Python DataFrame users, we have a Python wrapper\n",
    "around Scala DataFrames that avoids the Python sub-process/JVM communication\n",
    "overhead. Spark DataFrames has many performance enhancements through the\n",
    "Catalyst Optimizer and Project Tungsten which we have reviewed in this chapter.  \n",
    "\n",
    "In this chapter, we also reviewed how to work with Spark DataFrames and worked\n",
    "on an on-time flight performance scenario using DataFrames.\n",
    "In this chapter, we created and worked with DataFrames by generating the data or\n",
    "making use of existing datasets.  \n",
    "\n",
    "In the next chapter, we will discuss how to transform and understand your\n",
    "own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_envi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
